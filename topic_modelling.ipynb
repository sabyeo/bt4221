{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('scrape/restaurant-data/cleaned_restaurant_reviews.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "      <th>cleaned_categories</th>\n",
       "      <th>...</th>\n",
       "      <th>Accessible</th>\n",
       "      <th>Indian</th>\n",
       "      <th>Japanese</th>\n",
       "      <th>Noodles</th>\n",
       "      <th>Sustainable</th>\n",
       "      <th>Desserts</th>\n",
       "      <th>European</th>\n",
       "      <th>Bites</th>\n",
       "      <th>Alcohol</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nFish Ball Minced Meat Noodle\\nFishball, meat...</td>\n",
       "      <td>Triffany Lim</td>\n",
       "      <td>21m ago</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nOrh lua\\nThere are a couple of stores, but g...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:12pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nPeanut sauce was ace\\nI love a good satay pe...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:10pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nClassic BBQ wings\\nJuicy and tasty like it’s...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:09pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nBBQ stingray\\nIt was yummy but slight warnin...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:08pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box D  $13.80\\n川椒雞柳 | 鮮腐竹蝦球 | 清炒西蘭花 | ...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 27, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box C  $11.80\\n普寧豆醬走地雞 | 鮮菌翡翠豆腐 | 蒜茸炒四...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 26, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box B  $11.80\\n蒜子豆豉凉瓜黑豬梅肉 | 香菌扒豆腐 | 蒜茸...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 9, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\n潮州糜 Bento A  $12.80\\n鹵鴨拼豆干 | 川椒雞 | 欖菜四季苗| 菜脯...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 1, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nNgoh Hiang\\nSo good! Crispy exterior with a ...</td>\n",
       "      <td>Rachel Syj</td>\n",
       "      <td>Jan 17, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28302 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  url  \\\n",
       "0   https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "1   https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "2   https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "3   https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "4   https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "..                                                ...   \n",
       "14  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "15  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "16  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "17  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "30  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "\n",
       "                             name neighbourhood     price  \\\n",
       "0              85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "1              85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "2              85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "3              85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "4              85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "..                            ...           ...       ...   \n",
       "14  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "15  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "16  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "17  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "30  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "\n",
       "                                   categories  \\\n",
       "0   ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "1   ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "2   ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "3   ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "4   ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "..                                        ...   \n",
       "14             ['Chinese', 'Good For Groups']   \n",
       "15             ['Chinese', 'Good For Groups']   \n",
       "16             ['Chinese', 'Good For Groups']   \n",
       "17             ['Chinese', 'Good For Groups']   \n",
       "30             ['Chinese', 'Good For Groups']   \n",
       "\n",
       "                                               review          user  \\\n",
       "0   \\nFish Ball Minced Meat Noodle\\nFishball, meat...  Triffany Lim   \n",
       "1   \\nOrh lua\\nThere are a couple of stores, but g...      Ally Tan   \n",
       "2   \\nPeanut sauce was ace\\nI love a good satay pe...      Ally Tan   \n",
       "3   \\nClassic BBQ wings\\nJuicy and tasty like it’s...      Ally Tan   \n",
       "4   \\nBBQ stingray\\nIt was yummy but slight warnin...      Ally Tan   \n",
       "..                                                ...           ...   \n",
       "14  \\nBento Box D  $13.80\\n川椒雞柳 | 鮮腐竹蝦球 | 清炒西蘭花 | ...           K T   \n",
       "15  \\nBento Box C  $11.80\\n普寧豆醬走地雞 | 鮮菌翡翠豆腐 | 蒜茸炒四...           K T   \n",
       "16  \\nBento Box B  $11.80\\n蒜子豆豉凉瓜黑豬梅肉 | 香菌扒豆腐 | 蒜茸...           K T   \n",
       "17  \\n潮州糜 Bento A  $12.80\\n鹵鴨拼豆干 | 川椒雞 | 欖菜四季苗| 菜脯...           K T   \n",
       "30  \\nNgoh Hiang\\nSo good! Crispy exterior with a ...    Rachel Syj   \n",
       "\n",
       "                                date  cleaned_price  \\\n",
       "0                            21m ago            5.0   \n",
       "1                   Jul 30 at 4:12pm            5.0   \n",
       "2                   Jul 30 at 4:10pm            5.0   \n",
       "3                   Jul 30 at 4:09pm            5.0   \n",
       "4                   Jul 30 at 4:08pm            5.0   \n",
       "..                               ...            ...   \n",
       "14            Oct 27, 2020                     50.0   \n",
       "15            Oct 26, 2020                     50.0   \n",
       "16             Oct 9, 2020                     50.0   \n",
       "17             Oct 1, 2020                     50.0   \n",
       "30            Jan 17, 2020                     50.0   \n",
       "\n",
       "                       cleaned_categories  ...  Accessible  Indian  Japanese  \\\n",
       "0   ['Local Delights', 'Supper', 'Value']  ...           0       0         0   \n",
       "1   ['Local Delights', 'Supper', 'Value']  ...           0       0         0   \n",
       "2   ['Local Delights', 'Supper', 'Value']  ...           0       0         0   \n",
       "3   ['Local Delights', 'Supper', 'Value']  ...           0       0         0   \n",
       "4   ['Local Delights', 'Supper', 'Value']  ...           0       0         0   \n",
       "..                                    ...  ...         ...     ...       ...   \n",
       "14              ['Chinese', 'Accessible']  ...           1       0         0   \n",
       "15              ['Chinese', 'Accessible']  ...           1       0         0   \n",
       "16              ['Chinese', 'Accessible']  ...           1       0         0   \n",
       "17              ['Chinese', 'Accessible']  ...           1       0         0   \n",
       "30              ['Chinese', 'Accessible']  ...           1       0         0   \n",
       "\n",
       "    Noodles  Sustainable  Desserts  European  Bites  Alcohol   region  \n",
       "0         0            0         0         0      0        0     East  \n",
       "1         0            0         0         0      0        0     East  \n",
       "2         0            0         0         0      0        0     East  \n",
       "3         0            0         0         0      0        0     East  \n",
       "4         0            0         0         0      0        0     East  \n",
       "..      ...          ...       ...       ...    ...      ...      ...  \n",
       "14        0            0         0         0      0        0  Central  \n",
       "15        0            0         0         0      0        0  Central  \n",
       "16        0            0         0         0      0        0  Central  \n",
       "17        0            0         0         0      0        0  Central  \n",
       "30        0            0         0         0      0        0  Central  \n",
       "\n",
       "[28302 rows x 43 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decontraction\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(filename, deemojize=False, clean_punctuation=False, remove_stopwords=False, lemmatize=False, stemming=False):\n",
    "    df = pd.read_csv(filename, index_col=0)\n",
    "    text_list = df['review']\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "    add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "        \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "        'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "        'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "        'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "        'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "        'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "        's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "        've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "        \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "        'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "    add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "        'it','in','ve','well','could','would','really','also','even',\n",
    "        'alway','always','still','never','much','thing','yet',\n",
    "        'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "        'put','went','seem','order','ordered','give','eat','make','get']\n",
    "    \n",
    "    stopwords.extend(new_stopwords)\n",
    "    stopwords.extend(add_stopwords)\n",
    "    stopwords.extend(add_stopwords_2)\n",
    "\n",
    "    ### FOR SENTIMENT ANALYSIS< UNCOMMENT THIS\n",
    "    # not_stopwords = {'no','nor','not'} \n",
    "    # stopwords = set([word for word in stopwords if word not in not_stopwords])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_text_list = []\n",
    "    count = 0\n",
    "    for text in text_list:\n",
    "\n",
    "        # lower case\n",
    "        text = text.lower()\n",
    "    \n",
    "        if deemojize:\n",
    "            text = emoji.demojize(text)\n",
    "    \n",
    "        if clean_punctuation:\n",
    "            text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # translate to english\n",
    "        # text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    \n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "        if remove_stopwords:\n",
    "            tokens = [word for word in tokens if word not in stopwords]\n",
    "\n",
    "        if lemmatize:\n",
    "            # POS tagging\n",
    "            tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "            # lemmatization\n",
    "            tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "                if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "        if stemming:\n",
    "            tokens = [ps.stem(word) for word in tokens]\n",
    "    \n",
    "        # concatenate tokens back\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "        if count%100 == 0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "\n",
    "    df['cleaned_text'] = cleaned_text_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_text('scrape/restaurant-data/cleaned_restaurant_reviews.csv', deemojize=True, clean_punctuation=True, remove_stopwords=True, lemmatize=True, stemming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('restaurant_topic_modelling_text_cleaned.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  cleaned_df = pd.read_csv('restaurant_topic_modelling_text_cleaned.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df = cleaned_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "docs = cleaned_df['cleaned_text']\n",
    "processed_docs = [d.split() for d in docs]\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term document frequency\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=1, step=1):\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=id2word,random_seed=101)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = gensim.models.CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mallet_path = 'mallet-2.0.8/bin/mallet'\n",
    "#ldamallet = gensim.models.LdaMallet(mallet_path, corpus=bow_corpus, num_topics=10, id2word=dictionary)\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=10)\n",
    "\n",
    "# Show Topics\n",
    "pprint(lda_model.show_topics(num_topics=-1, formatted=False))\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = gensim.models.CoherenceModel(model=lda_model, texts=docs, dictionary=dictionary, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, corpus=corpus, texts=data_lemmatized, start=2, limit=10, step=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Show graph\n",
    "limit=10; start=1; step=2;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "# number of topics\n",
    "num_topics = 3\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=num_topics)\n",
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics\n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('ldavis_prepared_'+str(num_topics))\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'ldavis_prepared_'+ str(num_topics) +'.html')\n",
    "LDAvis_prepared"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
