{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pprint import pprint\n",
    "\n",
    "import pyLDAvis.gensim\n",
    "import pickle \n",
    "import pyLDAvis\n",
    "import os\n",
    "import numpy as np\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('scrape/restaurant-data/cleaned_restaurant_reviews.csv', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "      <th>cleaned_categories</th>\n",
       "      <th>...</th>\n",
       "      <th>Western</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Sustainable</th>\n",
       "      <th>Novel</th>\n",
       "      <th>Desserts</th>\n",
       "      <th>Bites</th>\n",
       "      <th>Supper</th>\n",
       "      <th>Breakfast &amp; Brunch</th>\n",
       "      <th>Halal</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nFish Ball Minced Meat Noodle\\nFishball, meat...</td>\n",
       "      <td>Triffany Lim</td>\n",
       "      <td>21m ago</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nOrh lua\\nThere are a couple of stores, but g...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:12pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nPeanut sauce was ace\\nI love a good satay pe...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:10pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nClassic BBQ wings\\nJuicy and tasty like it’s...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:09pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nBBQ stingray\\nIt was yummy but slight warnin...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:08pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28294</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box D  $13.80\\n川椒雞柳 | 鮮腐竹蝦球 | 清炒西蘭花 | ...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 27, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28295</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box C  $11.80\\n普寧豆醬走地雞 | 鮮菌翡翠豆腐 | 蒜茸炒四...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 26, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28296</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nBento Box B  $11.80\\n蒜子豆豉凉瓜黑豬梅肉 | 香菌扒豆腐 | 蒜茸...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 9, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28297</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\n潮州糜 Bento A  $12.80\\n鹵鴨拼豆干 | 川椒雞 | 欖菜四季苗| 菜脯...</td>\n",
       "      <td>K T</td>\n",
       "      <td>Oct 1, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28298</th>\n",
       "      <td>https://www.burpple.com/chui-huay-lim-teochew-...</td>\n",
       "      <td>Chui Huay Lim Teochew Cuisine</td>\n",
       "      <td>Newton</td>\n",
       "      <td>~$50/pax</td>\n",
       "      <td>['Chinese', 'Good For Groups']</td>\n",
       "      <td>\\nNgoh Hiang\\nSo good! Crispy exterior with a ...</td>\n",
       "      <td>Rachel Syj</td>\n",
       "      <td>Jan 17, 2020</td>\n",
       "      <td>50.0</td>\n",
       "      <td>['Chinese', 'Accessible']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Central</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>28299 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     url  \\\n",
       "0      https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "1      https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "2      https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "3      https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "4      https://www.burpple.com/bedok-85-market?bp_ref...   \n",
       "...                                                  ...   \n",
       "28294  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "28295  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "28296  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "28297  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "28298  https://www.burpple.com/chui-huay-lim-teochew-...   \n",
       "\n",
       "                                name neighbourhood     price  \\\n",
       "0                 85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "1                 85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "2                 85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "3                 85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "4                 85 Fengshan Centre         Bedok   ~$5/pax   \n",
       "...                              ...           ...       ...   \n",
       "28294  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "28295  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "28296  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "28297  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "28298  Chui Huay Lim Teochew Cuisine        Newton  ~$50/pax   \n",
       "\n",
       "                                      categories  \\\n",
       "0      ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "1      ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "2      ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "3      ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "4      ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "...                                          ...   \n",
       "28294             ['Chinese', 'Good For Groups']   \n",
       "28295             ['Chinese', 'Good For Groups']   \n",
       "28296             ['Chinese', 'Good For Groups']   \n",
       "28297             ['Chinese', 'Good For Groups']   \n",
       "28298             ['Chinese', 'Good For Groups']   \n",
       "\n",
       "                                                  review          user  \\\n",
       "0      \\nFish Ball Minced Meat Noodle\\nFishball, meat...  Triffany Lim   \n",
       "1      \\nOrh lua\\nThere are a couple of stores, but g...      Ally Tan   \n",
       "2      \\nPeanut sauce was ace\\nI love a good satay pe...      Ally Tan   \n",
       "3      \\nClassic BBQ wings\\nJuicy and tasty like it’s...      Ally Tan   \n",
       "4      \\nBBQ stingray\\nIt was yummy but slight warnin...      Ally Tan   \n",
       "...                                                  ...           ...   \n",
       "28294  \\nBento Box D  $13.80\\n川椒雞柳 | 鮮腐竹蝦球 | 清炒西蘭花 | ...           K T   \n",
       "28295  \\nBento Box C  $11.80\\n普寧豆醬走地雞 | 鮮菌翡翠豆腐 | 蒜茸炒四...           K T   \n",
       "28296  \\nBento Box B  $11.80\\n蒜子豆豉凉瓜黑豬梅肉 | 香菌扒豆腐 | 蒜茸...           K T   \n",
       "28297  \\n潮州糜 Bento A  $12.80\\n鹵鴨拼豆干 | 川椒雞 | 欖菜四季苗| 菜脯...           K T   \n",
       "28298  \\nNgoh Hiang\\nSo good! Crispy exterior with a ...    Rachel Syj   \n",
       "\n",
       "                                   date  cleaned_price  \\\n",
       "0                               21m ago            5.0   \n",
       "1                      Jul 30 at 4:12pm            5.0   \n",
       "2                      Jul 30 at 4:10pm            5.0   \n",
       "3                      Jul 30 at 4:09pm            5.0   \n",
       "4                      Jul 30 at 4:08pm            5.0   \n",
       "...                                 ...            ...   \n",
       "28294            Oct 27, 2020                     50.0   \n",
       "28295            Oct 26, 2020                     50.0   \n",
       "28296             Oct 9, 2020                     50.0   \n",
       "28297             Oct 1, 2020                     50.0   \n",
       "28298            Jan 17, 2020                     50.0   \n",
       "\n",
       "                          cleaned_categories  ... Western  Recommended  \\\n",
       "0      ['Local Delights', 'Supper', 'Value']  ...       0            0   \n",
       "1      ['Local Delights', 'Supper', 'Value']  ...       0            0   \n",
       "2      ['Local Delights', 'Supper', 'Value']  ...       0            0   \n",
       "3      ['Local Delights', 'Supper', 'Value']  ...       0            0   \n",
       "4      ['Local Delights', 'Supper', 'Value']  ...       0            0   \n",
       "...                                      ...  ...     ...          ...   \n",
       "28294              ['Chinese', 'Accessible']  ...       0            0   \n",
       "28295              ['Chinese', 'Accessible']  ...       0            0   \n",
       "28296              ['Chinese', 'Accessible']  ...       0            0   \n",
       "28297              ['Chinese', 'Accessible']  ...       0            0   \n",
       "28298              ['Chinese', 'Accessible']  ...       0            0   \n",
       "\n",
       "       Sustainable  Novel  Desserts  Bites  Supper  Breakfast & Brunch  Halal  \\\n",
       "0                0      0         0      0       1                   0      0   \n",
       "1                0      0         0      0       1                   0      0   \n",
       "2                0      0         0      0       1                   0      0   \n",
       "3                0      0         0      0       1                   0      0   \n",
       "4                0      0         0      0       1                   0      0   \n",
       "...            ...    ...       ...    ...     ...                 ...    ...   \n",
       "28294            0      0         0      0       0                   0      0   \n",
       "28295            0      0         0      0       0                   0      0   \n",
       "28296            0      0         0      0       0                   0      0   \n",
       "28297            0      0         0      0       0                   0      0   \n",
       "28298            0      0         0      0       0                   0      0   \n",
       "\n",
       "        region  \n",
       "0         East  \n",
       "1         East  \n",
       "2         East  \n",
       "3         East  \n",
       "4         East  \n",
       "...        ...  \n",
       "28294  Central  \n",
       "28295  Central  \n",
       "28296  Central  \n",
       "28297  Central  \n",
       "28298  Central  \n",
       "\n",
       "[28299 rows x 44 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "import nltk\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "    'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "    's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "    \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "    'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "    'it','in','ve','well','could','would','really','also','even',\n",
    "    'alway','always','still','never','much','thing','yet',\n",
    "    'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "    'put','went','seem','order','ordered','give','eat','make','get']\n",
    "\n",
    "stopwords.extend(new_stopwords)\n",
    "stopwords.extend(add_stopwords)\n",
    "stopwords.extend(add_stopwords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n"
     ]
    }
   ],
   "source": [
    "cleaned_review_list = []\n",
    "count = 0\n",
    "\n",
    "for review in data['review']:\n",
    "    # lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # deemojize\n",
    "    review = emoji.demojize(review)\n",
    "\n",
    "    # remove headers\n",
    "    review = ' '.join(review.split('\\n')[2:])\n",
    "\n",
    "    # remove location (pushpin or location:)\n",
    "    review = review.split('round_pushpin')[0]\n",
    "    review = review.split('location:')[0]\n",
    "\n",
    "    # clean punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(review)\n",
    "\n",
    "    # remove stopwords and URLs\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [word for word in tokens if 'http' not in word]\n",
    "    tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "    # decontraction\n",
    "    tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "    ### to add in translation code... shld translate each token\n",
    "\n",
    "    # lemmatizing ###### IDK if we shld lemmatize or not\n",
    "    # POS tagging\n",
    "    tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "    # lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "        if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "\n",
    "    # concatenate tokens back\n",
    "    cleaned_review = \" \".join(tokens)\n",
    "    cleaned_review_list.append(cleaned_review)\n",
    "\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the old cleaned text\n",
    "data['cleaned_text'] = cleaned_review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df = data[['url', 'cleaned_text']]\n",
    "restaurant_review_df = restaurant_review_df.groupby(['url'], as_index = False).agg({'cleaned_text': ' '.join})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/108-matcha-saro?bp_ref...</td>\n",
       "      <td>s850 strong matcha flavor soft serve topped wh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/136-hong-kong-street-f...</td>\n",
       "      <td>zhi char feast family love tze char lose count...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.burpple.com/15-stamford?bp_ref=%2F...</td>\n",
       "      <td>four hand dinner chef alvin chef jay legendary...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.burpple.com/25-degrees-singapore?b...</td>\n",
       "      <td>oh yesh loveeeee burger goooooood locate hotel...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.burpple.com/46-mittsu?bp_ref=%2Ff%...</td>\n",
       "      <td>signature firebird koreaninspired sandwich fry...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>840</th>\n",
       "      <td>https://www.burpple.com/yun-nans?bp_ref=%2Ff%2...</td>\n",
       "      <td>stir fry yunnan rice noodle assort seafood dee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841</th>\n",
       "      <td>https://www.burpple.com/zafferano?bp_ref=%2Ff%...</td>\n",
       "      <td>portion spaghetti dress tomato extraction oliv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842</th>\n",
       "      <td>https://www.burpple.com/zai-shun-curry-fish-he...</td>\n",
       "      <td>favourite cold weather food hearty comfort dis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>843</th>\n",
       "      <td>https://www.burpple.com/zazz-pizza?bp_ref=%2Ff...</td>\n",
       "      <td>beside pizza try braise veel cheek pappardelle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>844</th>\n",
       "      <td>https://www.burpple.com/zoeys-diner?bp_ref=%2F...</td>\n",
       "      <td>happen zoeys drink sound excite neither rock m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>845 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   url  \\\n",
       "0    https://www.burpple.com/108-matcha-saro?bp_ref...   \n",
       "1    https://www.burpple.com/136-hong-kong-street-f...   \n",
       "2    https://www.burpple.com/15-stamford?bp_ref=%2F...   \n",
       "3    https://www.burpple.com/25-degrees-singapore?b...   \n",
       "4    https://www.burpple.com/46-mittsu?bp_ref=%2Ff%...   \n",
       "..                                                 ...   \n",
       "840  https://www.burpple.com/yun-nans?bp_ref=%2Ff%2...   \n",
       "841  https://www.burpple.com/zafferano?bp_ref=%2Ff%...   \n",
       "842  https://www.burpple.com/zai-shun-curry-fish-he...   \n",
       "843  https://www.burpple.com/zazz-pizza?bp_ref=%2Ff...   \n",
       "844  https://www.burpple.com/zoeys-diner?bp_ref=%2F...   \n",
       "\n",
       "                                          cleaned_text  \n",
       "0    s850 strong matcha flavor soft serve topped wh...  \n",
       "1    zhi char feast family love tze char lose count...  \n",
       "2    four hand dinner chef alvin chef jay legendary...  \n",
       "3    oh yesh loveeeee burger goooooood locate hotel...  \n",
       "4    signature firebird koreaninspired sandwich fry...  \n",
       "..                                                 ...  \n",
       "840  stir fry yunnan rice noodle assort seafood dee...  \n",
       "841  portion spaghetti dress tomato extraction oliv...  \n",
       "842  favourite cold weather food hearty comfort dis...  \n",
       "843  beside pizza try braise veel cheek pappardelle...  \n",
       "844  happen zoeys drink sound excite neither rock m...  \n",
       "\n",
       "[845 rows x 2 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "restaurant_review_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dictionary\n",
    "docs = data['cleaned_text'] ###\n",
    "processed_docs = [d.split() for d in docs]\n",
    "dictionary = gensim.corpora.Dictionary(processed_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# term document frequency\n",
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                       id2word=dictionary,\n",
    "                                       num_topics=10,\n",
    "                                       alpha=0.1, # document topic density. higher alpha, documents composed of more topics\n",
    "                                       eta=0.01, # topic word density. higher beta, topics composed of large number of words in the corpus\n",
    "                                       chunksize=100, # number of documents to consider at once\n",
    "                                       passes=10, # number of times to go through the entire corpus\n",
    "                                       random_state =100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.035*\"pork\" + 0.017*\"meat\" + 0.013*\"tender\" + 0.012*\"rib\" + 0.012*\"rice\" + '\n",
      "  '0.012*\"sauce\" + 0.010*\"belly\" + 0.009*\"grill\" + 0.009*\"dish\" + 0.008*\"fry\"'),\n",
      " (1,\n",
      "  '0.017*\"menu\" + 0.011*\"sauce\" + 0.011*\"beef\" + 0.010*\"restaurant\" + '\n",
      "  '0.010*\"dish\" + 0.009*\"serve\" + 0.009*\"chef\" + 0.009*\"available\" + '\n",
      "  '0.008*\"set\" + 0.007*\"new\"'),\n",
      " (2,\n",
      "  '0.010*\"food\" + 0.009*\"dish\" + 0.009*\"time\" + 0.008*\"noodle\" + 0.007*\"come\" '\n",
      "  '+ 0.007*\"try\" + 0.007*\"u\" + 0.006*\"set\" + 0.006*\"soup\" + 0.006*\"place\"'),\n",
      " (3,\n",
      "  '0.023*\"good\" + 0.015*\"taste\" + 0.012*\"like\" + 0.011*\"quite\" + 0.011*\"dish\" '\n",
      "  '+ 0.010*\"pretty\" + 0.010*\"food\" + 0.009*\"portion\" + 0.009*\"try\" + '\n",
      "  '0.009*\"fry\"'),\n",
      " (4,\n",
      "  '0.018*\"tea\" + 0.016*\"coffee\" + 0.015*\"taste\" + 0.014*\"sweet\" + '\n",
      "  '0.012*\"matcha\" + 0.011*\"like\" + 0.011*\"drink\" + 0.010*\"flavour\" + '\n",
      "  '0.009*\"milk\" + 0.009*\"cream\"'),\n",
      " (5,\n",
      "  '0.043*\"cream\" + 0.036*\"chocolate\" + 0.029*\"cake\" + 0.025*\"ice\" + '\n",
      "  '0.013*\"sweet\" + 0.012*\"dessert\" + 0.010*\"vanilla\" + 0.009*\"waffle\" + '\n",
      "  '0.009*\"croissant\" + 0.009*\"dark\"'),\n",
      " (6,\n",
      "  '0.017*\"good\" + 0.015*\"like\" + 0.015*\"bread\" + 0.013*\"try\" + 0.011*\"egg\" + '\n",
      "  '0.009*\"pizza\" + 0.009*\"taste\" + 0.008*\"quite\" + 0.007*\"price\" + '\n",
      "  '0.007*\"fill\"'),\n",
      " (7,\n",
      "  '0.019*\"fry\" + 0.018*\"sauce\" + 0.018*\"flavour\" + 0.016*\"sweet\" + '\n",
      "  '0.013*\"chicken\" + 0.011*\"crispy\" + 0.010*\"savoury\" + 0.009*\"dish\" + '\n",
      "  '0.009*\"egg\" + 0.009*\"soft\"'),\n",
      " (8,\n",
      "  '0.013*\"flavour\" + 0.010*\"tart\" + 0.010*\"sweet\" + 0.007*\"like\" + '\n",
      "  '0.006*\"cake\" + 0.005*\"taste\" + 0.005*\"dessert\" + 0.005*\"pastry\" + '\n",
      "  '0.005*\"light\" + 0.005*\"make\"'),\n",
      " (9,\n",
      "  '0.025*\"prawn\" + 0.017*\"noodle\" + 0.014*\"egg\" + 0.013*\"pasta\" + 0.012*\"dish\" '\n",
      "  '+ 0.011*\"soup\" + 0.011*\"pork\" + 0.011*\"good\" + 0.010*\"food\" + 0.009*\"bowl\"')]\n"
     ]
    }
   ],
   "source": [
    "# Print the Keyword in the 10 topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "lda_score = coherence_model_lda.get_coherence()\n",
    "lda_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(k):\n",
    "    \n",
    "    lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=k,\n",
    "                                           alpha=0.1, # document topic density. higher alpha, documents composed of more topics\n",
    "                                           eta=0.01, # topic word density. higher beta, topics composed of large number of words in the corpus\n",
    "                                           chunksize=100, # number of documents to consider at once\n",
    "                                           passes=10, # number of times to go through the entire corpus\n",
    "                                           random_state =100)\n",
    "    \n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=processed_docs, dictionary=dictionary, coherence='c_v')\n",
    "    \n",
    "    return coherence_model_lda.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# grid = {}\n",
    "# grid['Validation_Set'] = {}\n",
    "\n",
    "# # Topics range\n",
    "# min_topics = 2\n",
    "# max_topics = 11\n",
    "# step_size = 1\n",
    "# topics_range = range(min_topics, max_topics, step_size)\n",
    "\n",
    "# # Alpha parameter\n",
    "# alpha = list(np.arange(0.01, 1, 0.3))\n",
    "# alpha.append('symmetric')\n",
    "# alpha.append('asymmetric')\n",
    "\n",
    "# # Beta parameter\n",
    "# beta = list(np.arange(0.01, 1, 0.3))\n",
    "# beta.append('symmetric')\n",
    "\n",
    "\n",
    "# model_results = {\n",
    "#                  'Topics': [],\n",
    "#                  'Alpha': [],\n",
    "#                  'Beta': [],\n",
    "#                  'Coherence': []\n",
    "#                 }\n",
    "\n",
    "# # Can take a long time to run\n",
    "# if 1 == 1:\n",
    "#     pbar = tqdm.tqdm(total=(len(beta)*len(alpha)*len(topics_range)))\n",
    "    \n",
    "#     # iterate through number of topics\n",
    "#     for k in topics_range:\n",
    "#         # iterate through alpha values\n",
    "#         for a in alpha:\n",
    "#             # iterare through beta values\n",
    "#             for b in beta:\n",
    "#                 # get the coherence score for the given parameters\n",
    "#                 cv = compute_coherence_values(k=k, a=a, b=b)\n",
    "#                 # Save the model results\n",
    "#                 model_results['Topics'].append(k)\n",
    "#                 model_results['Alpha'].append(a)\n",
    "#                 model_results['Beta'].append(b)\n",
    "#                 model_results['Coherence'].append(cv)\n",
    "                \n",
    "#                 pbar.update(1)\n",
    "#     pd.DataFrame(model_results).to_csv('restaurant_lda_tuning_results.csv', index=False)\n",
    "#     pbar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate through number of topics\n",
    "coherence_values = []\n",
    "topics_range = range(2,11,1)\n",
    "\n",
    "for k in topics_range:\n",
    "    value = compute_coherence_values(k)\n",
    "    print(k)\n",
    "    print(value)\n",
    "    coherence_values.append(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show graph\n",
    "plt.plot(topics_range, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()\n",
    "\n",
    "# choose num topics == 6 instead since 6 & 7 is stable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimal parameters\n",
    "num_topics = 3 ### CHANGE THIS\n",
    "\n",
    "# Build LDA model\n",
    "lda_model = gensim.models.LdaMulticore(corpus=bow_corpus,\n",
    "                                        id2word=dictionary,\n",
    "                                        num_topics=num_topics,\n",
    "                                        alpha=0.1, # document topic density. higher alpha, documents composed of more topics\n",
    "                                        eta=0.01, # topic word density. higher beta, topics composed of large number of words in the corpus\n",
    "                                        chunksize=100, # number of documents to consider at once\n",
    "                                        passes=10, # number of times to go through the entire corpus\n",
    "                                        random_state =100)\n",
    "# Print the Keyword in the topics\n",
    "pprint(lda_model.print_topics())\n",
    "doc_lda = lda_model[bow_corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the topics \n",
    "pyLDAvis.enable_notebook()\n",
    "LDAvis_data_filepath = os.path.join('restaurant_ldavis_prepared_'+str(num_topics)+'_new_cleaning_lemmatized')\n",
    "# # this is a bit time consuming - make the if statement True\n",
    "# # if you want to execute visualization prep yourself\n",
    "if 1 == 1:\n",
    "    LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, bow_corpus, dictionary)\n",
    "    with open(LDAvis_data_filepath, 'wb') as f:\n",
    "        pickle.dump(LDAvis_prepared, f)\n",
    "# load the pre-prepared pyLDAvis data from disk\n",
    "with open(LDAvis_data_filepath, 'rb') as f:\n",
    "    LDAvis_prepared = pickle.load(f)\n",
    "pyLDAvis.save_html(LDAvis_prepared, 'restaurant_ldavis_prepared_'+ str(num_topics) +'_new_cleaning_lemmatized.html')\n",
    "LDAvis_prepared\n",
    "\n",
    "### https://we1s.ucsb.edu/research/we1s-tools-and-software/topic-model-observatory/tmo-guide/tmo-guide-pyldavis/\n",
    "# A “relevance metric” slider scale at the top of the right panel controls how the words for a topic are sorted.\n",
    "# lambda 1: sorts words by their frequency in the topic (red bars)\n",
    "# lambda 0: sorts words by their \"lift\". Words whose red bars are nearly as long as their blue bars will be at the top Lift means how much a word's frequency sticks out in a topic above the baseline of its overall frequency in the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
