{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# for data analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# for visualizations\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for data preparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "# imblean provides tools for us to deal with imbalanced class sizes\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# # For entropy computation\n",
    "# from pyitlib import discrete_random_variable as drv\n",
    "\n",
    "from scipy import stats\n",
    "import missingno\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TEXT EDA\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# import spacy\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "#import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100\n",
    "#import matplotlib.gridspec as gridspec\n",
    "#import seaborn as sns; sns.set()\n",
    "import ast\n",
    "\n",
    "from datetime import date, time, datetime\n",
    "import calendar\n",
    "\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file():\n",
    "    full_data = pd.DataFrame()\n",
    "    for file in os.listdir('../scrape/restaurant-data'):\n",
    "        if 'compiled' in file:\n",
    "            data = pd.read_csv(f'../scrape/restaurant-data/{file}', index_col = 0)\n",
    "            full_data = full_data.append(data)\n",
    "    return full_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_file()\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. reviews: 28731\n",
      "No. restaurants: 863\n"
     ]
    }
   ],
   "source": [
    "# Total Number of restaurant reviews\n",
    "print(f'No. reviews: {len(data)}')\n",
    "# Number of restaurants with reviews\n",
    "num = len(data['url'].unique())\n",
    "print(f'No. restaurants: {num}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Cost Column "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nFish Ball Minced Meat Noodle\\nFishball, meat...</td>\n",
       "      <td>Triffany Lim</td>\n",
       "      <td>21m ago</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nOrh lua\\nThere are a couple of stores, but g...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:12pm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nPeanut sauce was ace\\nI love a good satay pe...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:10pm</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url                name  \\\n",
       "0  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "1  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "2  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "\n",
       "  neighbourhood    price                                 categories  \\\n",
       "0         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "1         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "2         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "\n",
       "                                              review          user  \\\n",
       "0  \\nFish Ball Minced Meat Noodle\\nFishball, meat...  Triffany Lim   \n",
       "1  \\nOrh lua\\nThere are a couple of stores, but g...      Ally Tan   \n",
       "2  \\nPeanut sauce was ace\\nI love a good satay pe...      Ally Tan   \n",
       "\n",
       "               date  cleaned_price  \n",
       "0           21m ago            5.0  \n",
       "1  Jul 30 at 4:12pm            5.0  \n",
       "2  Jul 30 at 4:10pm            5.0  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['cleaned_price'] = data['price'].apply(lambda x: int(x.split('/')[0].replace('~$','')) if '$' in x else np.nan)\n",
    "data.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_mapping = {'Steak':['Western'], \n",
    " 'Cocktails':['Alcohol','Drinks'], \n",
    " 'Great View': ['Good Environment'], \n",
    " 'Sushi':['Japanese'], \n",
    " 'Ramen':['Japanese','Noodles'], \n",
    " 'Islandwide Delivery':['Convenient'], \n",
    " 'Craft Beer':['Drinks','Alcohol'], \n",
    " '1 For 1 Deals':['Value'],\n",
    " 'Teppanyaki':['Japanese'], \n",
    " 'Sustainable Dining on Beyond':['Sustainable'], \n",
    " 'Vegan friendly':['Sustainable'], \n",
    " 'Kopitiam':['Local Delights'], \n",
    " 'Char Kway Teow':['Local Delights','Noodles','Chinese'], \n",
    " 'Taiwanese':['Chinese'], \n",
    " 'Waffles':['Desserts'], \n",
    " 'Zi Char':['Local Delights','Chinese'], \n",
    " 'Fruit Tea':['Drinks'], \n",
    " 'Pasta':['Western','Noodles'], \n",
    " 'Vouchers':['Value'], \n",
    " 'Chirashi':['Japanese'], \n",
    " 'Bars':['Alcohol','Drinks'], \n",
    " 'Burpple Beyond Deals ðŸ’°':['Value'], \n",
    " 'BITES':['Bites'],\n",
    " 'Burpple Guides':['Recommended'],\n",
    " 'Michelin Guide Singapore 2018':['Recommended'],\n",
    "'Grill & BBQ':['Western','BBQ'], \n",
    "'Bread & Pastries':['Desserts','High Tea', 'Breakfast & Brunch'], \n",
    " 'Vegetarian friendly':['Sustainable'], \n",
    " 'Cheap & Good':['Value'], \n",
    " 'Mediterranean':['European'], \n",
    " 'Middle Eastern': ['European'],\n",
    " 'Michelin Guide Singapore 2017':['Recommended'], \n",
    " 'Hawker Food':['Local Delights'], \n",
    " 'Ice Cream & Yoghurt':['Desserts'], \n",
    " 'Cafes & Coffee':['Desserts','High Tea', 'Breakfast & Brunch'], \n",
    " 'Interesting':['Good Environment'],\n",
    "'Dinner with Drinks':['Drinks'], \n",
    "'Bak Kut Teh':['Local Delights', 'Chinese'],\n",
    "'Burgers':['Western'],\n",
    "'Korean Desserts': ['Desserts','Korean'],\n",
    " 'Vegetarian':['Sustainable'], \n",
    " 'Nasi Lemak':['South East Asian','Local Delights'], \n",
    " 'Salads':['Western'], \n",
    "'TAKEAWAY OPTION':['Convenient'],\n",
    "'Delivery':['Convenient'], \n",
    "'Sandwiches':['High Tea', 'Breakfast & Brunch', 'Western'], \n",
    "'Pizza':['Western'],\n",
    "'Vegan':['Sustainable'], \n",
    "'Dim Sum':['Chinese'], \n",
    "'Chicken Rice':['Local Delights','Chinese'], \n",
    "'Fried Chicken':['Korean', 'Western', 'Bites','Fast Food'], \n",
    "'Korean BBQ':['Korean','BBQ'],\n",
    "'Filipino Local Delights':['Filipino'], \n",
    "'Cakes':['Desserts','High Tea'], \n",
    "'Michelin Guide Singapore 2019':['Recommended'],\n",
    " 'Korean Fried Chicken':['Korean','Bites','Fast Food'], \n",
    "'Hot Pot': ['Chinese', 'Rainy Day Comforts'], \n",
    "'Soup': ['Rainy Day Comforts'],\n",
    "'Late Night':['Supper'],\n",
    " 'Bubble Tea':['Drinks'],\n",
    " 'BEYOND': ['Recommended', 'Value'],\n",
    " 'Argentinian':['European'],\n",
    " 'Filipino Local Delights': ['South East Asian'],\n",
    " 'Indonesian': ['South East Asian'],\n",
    " 'Malay': ['South East Asian'],\n",
    " 'Vietnamese':['South East Asian'],\n",
    " 'Peranakan': ['South East Asian'],\n",
    " 'Thai': ['South East Asian'],\n",
    " 'Greek':['European'],\n",
    " 'Russian': ['European'],\n",
    " 'Turkish': ['European'],\n",
    " 'Mexican': ['European'],\n",
    " 'Spanish': ['European'],\n",
    " 'Newly Opened': ['Novel'],\n",
    " 'Hidden Gem':['Novel'],\n",
    " 'French': ['European'],\n",
    " 'Italian': ['European'],\n",
    " 'Kid Friendly': ['Accessible'],\n",
    " 'Pet-Friendly': ['Accessible'],\n",
    " 'Good For Groups': ['Accessible']\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find unique categories \n",
    "def find_unique_categories(category_col_name, data):\n",
    "    all_categories = []\n",
    "    for categories in data[category_col_name]:\n",
    "        try:\n",
    "            all_categories.extend(ast.literal_eval(categories))\n",
    "        except:\n",
    "            all_categories.extend(categories)\n",
    "    print(f'Num of Categories:{len(set(all_categories))}')\n",
    "    for i in set(all_categories):\n",
    "        print(i)\n",
    "    return list(set(all_categories))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Categories:101\n",
      "Hot Pot\n",
      "Bread & Pastries\n",
      "Kopitiam\n",
      "Good For Groups\n",
      "Japanese\n",
      "Char Kway Teow\n",
      "Michelin Guide Singapore 2019\n",
      "Vegan friendly\n",
      "TAKEAWAY OPTION\n",
      "Vegetarian\n",
      "BEYOND\n",
      "Vietnamese\n",
      "Supper\n",
      "Taiwanese\n",
      "Korean\n",
      "Waffles\n",
      "Sandwiches\n",
      "Burpple Guides\n",
      "Craft Beer\n",
      "Islandwide Delivery\n",
      "Vegetarian friendly\n",
      "1 For 1 Deals\n",
      "Mexican\n",
      "Newly Opened\n",
      "Vegan\n",
      "Sushi\n",
      "Bites\n",
      "Late Night\n",
      "BBQ\n",
      "Soup\n",
      "Russian\n",
      "Michelin Guide Singapore 2018\n",
      "Chirashi\n",
      "Grill & BBQ\n",
      "Malay\n",
      "Kid Friendly\n",
      "Korean BBQ\n",
      "High Tea\n",
      "Chicken Rice\n",
      "Sustainable Dining on Beyond\n",
      "Hawker Food\n",
      "Bak Kut Teh\n",
      "Italian\n",
      "Halal\n",
      "Breakfast & Brunch\n",
      "Pasta\n",
      "Spanish\n",
      "Fast Food\n",
      "Zi Char\n",
      "Pizza\n",
      "Michelin Guide Singapore 2017\n",
      "Peranakan\n",
      "Western\n",
      "Burpple Beyond Deals ðŸ’°\n",
      "Greek\n",
      "Cheap & Good\n",
      "Korean Fried Chicken\n",
      "Dinner with Drinks\n",
      "Nasi Lemak\n",
      "Healthy\n",
      "Mediterranean\n",
      "Ice Cream & Yoghurt\n",
      "Cocktails\n",
      "Interesting\n",
      "Teppanyaki\n",
      "Middle Eastern\n",
      "Local Delights\n",
      "Salads\n",
      "Fruit Tea\n",
      "Cakes\n",
      "Thai\n",
      "Fine Dining\n",
      "French\n",
      "Desserts\n",
      "Burgers\n",
      "Steak\n",
      "Chinese\n",
      "Cafes & Coffee\n",
      "Delivery\n",
      "Buffets\n",
      "Bubble Tea\n",
      "Fried Chicken\n",
      "Hidden Gem\n",
      "European\n",
      "Rainy Day Comforts\n",
      "Turkish\n",
      "Filipino Local Delights\n",
      "Dim Sum\n",
      "Indian\n",
      "Ramen\n",
      "Great View\n",
      "Indonesian\n",
      "Date Night\n",
      "Vouchers\n",
      "BITES\n",
      "Seafood\n",
      "Korean Desserts\n",
      "Bars\n",
      "Noodles\n",
      "Argentinian\n",
      "Pet-Friendly\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hot Pot',\n",
       " 'Bread & Pastries',\n",
       " 'Kopitiam',\n",
       " 'Good For Groups',\n",
       " 'Japanese',\n",
       " 'Char Kway Teow',\n",
       " 'Michelin Guide Singapore 2019',\n",
       " 'Vegan friendly',\n",
       " 'TAKEAWAY OPTION',\n",
       " 'Vegetarian',\n",
       " 'BEYOND',\n",
       " 'Vietnamese',\n",
       " 'Supper',\n",
       " 'Taiwanese',\n",
       " 'Korean',\n",
       " 'Waffles',\n",
       " 'Sandwiches',\n",
       " 'Burpple Guides',\n",
       " 'Craft Beer',\n",
       " 'Islandwide Delivery',\n",
       " 'Vegetarian friendly',\n",
       " '1 For 1 Deals',\n",
       " 'Mexican',\n",
       " 'Newly Opened',\n",
       " 'Vegan',\n",
       " 'Sushi',\n",
       " 'Bites',\n",
       " 'Late Night',\n",
       " 'BBQ',\n",
       " 'Soup',\n",
       " 'Russian',\n",
       " 'Michelin Guide Singapore 2018',\n",
       " 'Chirashi',\n",
       " 'Grill & BBQ',\n",
       " 'Malay',\n",
       " 'Kid Friendly',\n",
       " 'Korean BBQ',\n",
       " 'High Tea',\n",
       " 'Chicken Rice',\n",
       " 'Sustainable Dining on Beyond',\n",
       " 'Hawker Food',\n",
       " 'Bak Kut Teh',\n",
       " 'Italian',\n",
       " 'Halal',\n",
       " 'Breakfast & Brunch',\n",
       " 'Pasta',\n",
       " 'Spanish',\n",
       " 'Fast Food',\n",
       " 'Zi Char',\n",
       " 'Pizza',\n",
       " 'Michelin Guide Singapore 2017',\n",
       " 'Peranakan',\n",
       " 'Western',\n",
       " 'Burpple Beyond Deals ðŸ’°',\n",
       " 'Greek',\n",
       " 'Cheap & Good',\n",
       " 'Korean Fried Chicken',\n",
       " 'Dinner with Drinks',\n",
       " 'Nasi Lemak',\n",
       " 'Healthy',\n",
       " 'Mediterranean',\n",
       " 'Ice Cream & Yoghurt',\n",
       " 'Cocktails',\n",
       " 'Interesting',\n",
       " 'Teppanyaki',\n",
       " 'Middle Eastern',\n",
       " 'Local Delights',\n",
       " 'Salads',\n",
       " 'Fruit Tea',\n",
       " 'Cakes',\n",
       " 'Thai',\n",
       " 'Fine Dining',\n",
       " 'French',\n",
       " 'Desserts',\n",
       " 'Burgers',\n",
       " 'Steak',\n",
       " 'Chinese',\n",
       " 'Cafes & Coffee',\n",
       " 'Delivery',\n",
       " 'Buffets',\n",
       " 'Bubble Tea',\n",
       " 'Fried Chicken',\n",
       " 'Hidden Gem',\n",
       " 'European',\n",
       " 'Rainy Day Comforts',\n",
       " 'Turkish',\n",
       " 'Filipino Local Delights',\n",
       " 'Dim Sum',\n",
       " 'Indian',\n",
       " 'Ramen',\n",
       " 'Great View',\n",
       " 'Indonesian',\n",
       " 'Date Night',\n",
       " 'Vouchers',\n",
       " 'BITES',\n",
       " 'Seafood',\n",
       " 'Korean Desserts',\n",
       " 'Bars',\n",
       " 'Noodles',\n",
       " 'Argentinian',\n",
       " 'Pet-Friendly']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "find_unique_categories('categories', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map categories\n",
    "all_new_categories = []\n",
    "for idx, row in data.iterrows():\n",
    "    categories = ast.literal_eval(row['categories'])\n",
    "    new_categories = []\n",
    "    for category in categories:\n",
    "        if category.strip() in category_mapping.keys():\n",
    "            new_categories.extend(category_mapping[category.strip()])\n",
    "        else:\n",
    "            new_categories.append(category)\n",
    "    all_new_categories.append(new_categories)\n",
    "\n",
    "data['cleaned_categories'] = all_new_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Categories:32\n",
      "Chinese\n",
      "Western\n",
      "Recommended\n",
      "Buffets\n",
      "Convenient\n",
      "Bites\n",
      "Japanese\n",
      "Rainy Day Comforts\n",
      "European\n",
      "BBQ\n",
      "Good Environment\n",
      "Healthy\n",
      "Indian\n",
      "Drinks\n",
      "South East Asian\n",
      "Novel\n",
      "Date Night\n",
      "Supper\n",
      "Local Delights\n",
      "High Tea\n",
      "Sustainable\n",
      "Korean\n",
      "Value\n",
      "Seafood\n",
      "Halal\n",
      "Breakfast & Brunch\n",
      "Noodles\n",
      "Alcohol\n",
      "Fast Food\n",
      "Fine Dining\n",
      "Accessible\n",
      "Desserts\n"
     ]
    }
   ],
   "source": [
    "# find unique categories after mapping\n",
    "list_cleaned_categories = find_unique_categories('cleaned_categories', data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decontraction\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    text_list = df['review']\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "    add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "        \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "        'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "        'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "        'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "        'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "        'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "        's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "        've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "        \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "        'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "    add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "        'it','in','ve','well','could','would','really','also','even',\n",
    "        'alway','always','still','never','much','thing','yet',\n",
    "        'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "        'put','went','seem','order','ordered','give','eat','make','get']\n",
    "    \n",
    "    stopwords.extend(new_stopwords)\n",
    "    stopwords.extend(add_stopwords)\n",
    "    stopwords.extend(add_stopwords_2)\n",
    "\n",
    "    ### FOR SENTIMENT ANALYSIS< UNCOMMENT THIS\n",
    "    # not_stopwords = {'no','nor','not'} \n",
    "    # stopwords = set([word for word in stopwords if word not in not_stopwords])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_text_list = []\n",
    "    count = 0\n",
    "    for text in text_list:\n",
    "\n",
    "        # lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # demojize    \n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "        # remove headers\n",
    "        text = ' '.join(text.split('\\n')[2:])\n",
    "\n",
    "        # remove location (pushpin or location:)\n",
    "        text = text.split('round_pushpin')[0]\n",
    "        text = text.split('location:')[0]\n",
    "    \n",
    "        # clean punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # remove stopwords and urls\n",
    "        tokens = [word for word in tokens if word not in stopwords]\n",
    "        tokens = [word for word in tokens if 'http' not in word]\n",
    "        tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "        # decontraction\n",
    "        tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "        # translate to english\n",
    "        # text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    \n",
    "        # POS tagging\n",
    "        tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "\n",
    "        # lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "            if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "        # concatenate tokens back\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "        if count%1000 == 0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "\n",
    "    df['cleaned_text'] = cleaned_text_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_text(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty strings with np.nan\n",
    "cleaned_df = cleaned_df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "      <th>cleaned_categories</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>28731.0</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28708</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28302</td>\n",
       "      <td>28731.0</td>\n",
       "      <td>28503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>429</td>\n",
       "      <td>NaN</td>\n",
       "      <td>228</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           url     name  neighbourhood    price  categories   review     user  \\\n",
       "False  28731.0  28731.0          28708  28731.0     28731.0  28731.0  28731.0   \n",
       "True       NaN      NaN             23      NaN         NaN      NaN      NaN   \n",
       "\n",
       "          date  cleaned_price  cleaned_categories  cleaned_text  \n",
       "False  28731.0          28302             28731.0         28503  \n",
       "True       NaN            429                 NaN           228  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is any null value\n",
    "cleaned_df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89 reviews have no neighbourhood. Thus, we drop these reviews.\n",
    "cleaned_data = cleaned_df[~cleaned_df['neighbourhood'].isna()]\n",
    "\n",
    "# 1781 reviews have no price. Thus, we drop these reviews.\n",
    "cleaned_data = cleaned_data[~cleaned_data['cleaned_price'].isna()]\n",
    "\n",
    "# 2 reviews have no review text. Thus, we drop these reviews.\n",
    "cleaned_data = cleaned_data[~cleaned_data['cleaned_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. reviews: 28075\n",
      "No. restaurants: 845\n"
     ]
    }
   ],
   "source": [
    "# Total Number of restaurant reviews\n",
    "print(f'No. reviews: {len(cleaned_data)}')\n",
    "# Number of restaurants with reviews\n",
    "num = len(cleaned_data['url'].unique())\n",
    "print(f'No. restaurants: {num}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Cat Variables to Binary Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of Categories:32\n",
      "Chinese\n",
      "Western\n",
      "Recommended\n",
      "Buffets\n",
      "Convenient\n",
      "Bites\n",
      "Japanese\n",
      "Rainy Day Comforts\n",
      "European\n",
      "BBQ\n",
      "Good Environment\n",
      "Healthy\n",
      "Indian\n",
      "Drinks\n",
      "South East Asian\n",
      "Novel\n",
      "Date Night\n",
      "Supper\n",
      "Local Delights\n",
      "High Tea\n",
      "Sustainable\n",
      "Korean\n",
      "Value\n",
      "Seafood\n",
      "Halal\n",
      "Breakfast & Brunch\n",
      "Noodles\n",
      "Alcohol\n",
      "Fast Food\n",
      "Fine Dining\n",
      "Accessible\n",
      "Desserts\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Chinese',\n",
       " 'Western',\n",
       " 'Recommended',\n",
       " 'Buffets',\n",
       " 'Convenient',\n",
       " 'Bites',\n",
       " 'Japanese',\n",
       " 'Rainy Day Comforts',\n",
       " 'European',\n",
       " 'BBQ',\n",
       " 'Good Environment',\n",
       " 'Healthy',\n",
       " 'Indian',\n",
       " 'Drinks',\n",
       " 'South East Asian',\n",
       " 'Novel',\n",
       " 'Date Night',\n",
       " 'Supper',\n",
       " 'Local Delights',\n",
       " 'High Tea',\n",
       " 'Sustainable',\n",
       " 'Korean',\n",
       " 'Value',\n",
       " 'Seafood',\n",
       " 'Halal',\n",
       " 'Breakfast & Brunch',\n",
       " 'Noodles',\n",
       " 'Alcohol',\n",
       " 'Fast Food',\n",
       " 'Fine Dining',\n",
       " 'Accessible',\n",
       " 'Desserts']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_cleaned_categories = find_unique_categories('cleaned_categories', cleaned_data)\n",
    "list_cleaned_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append empty columns with category names\n",
    "cleaned_data = cleaned_data.reindex(cleaned_data.columns.tolist() + list_cleaned_categories, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chinese\n",
      "Western\n",
      "Recommended\n",
      "Buffets\n",
      "Convenient\n",
      "Bites\n",
      "Japanese\n",
      "Rainy Day Comforts\n",
      "European\n",
      "BBQ\n",
      "Good Environment\n",
      "Healthy\n",
      "Indian\n",
      "Drinks\n",
      "South East Asian\n",
      "Novel\n",
      "Date Night\n",
      "Supper\n",
      "Local Delights\n",
      "High Tea\n",
      "Sustainable\n",
      "Korean\n",
      "Value\n",
      "Seafood\n",
      "Halal\n",
      "Breakfast & Brunch\n",
      "Noodles\n",
      "Alcohol\n",
      "Fast Food\n",
      "Fine Dining\n",
      "Accessible\n",
      "Desserts\n"
     ]
    }
   ],
   "source": [
    "for category in list_cleaned_categories:\n",
    "    print(category)\n",
    "    cleaned_data[category] = cleaned_data['cleaned_categories'].apply(lambda x: 1 if category in x else 0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mapping Neighbourhoods to Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'neighbourhood_to_region_mapping.xlsx'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m neighbourhood_to_region_mapping \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_excel(\u001b[39m'\u001b[39;49m\u001b[39mneighbourhood_to_region_mapping.xlsx\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      2\u001b[0m neighbourhood_to_region_mapping[\u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m neighbourhood_to_region_mapping[\u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: x\u001b[39m.\u001b[39mreplace(\u001b[39m'\u001b[39m\u001b[39m\\xa0\u001b[39;00m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m))\n\u001b[1;32m      3\u001b[0m neighbourhood_to_region_mapping \u001b[39m=\u001b[39m neighbourhood_to_region_mapping\u001b[39m.\u001b[39mset_index(\u001b[39m'\u001b[39m\u001b[39mneighbourhood\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mT\u001b[39m.\u001b[39mto_dict(\u001b[39m'\u001b[39m\u001b[39mrecords\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/util/_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[39m=\u001b[39m new_arg_value\n\u001b[0;32m--> 211\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[39m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/io/excel/_base.py:482\u001b[0m, in \u001b[0;36mread_excel\u001b[0;34m(io, sheet_name, header, names, index_col, usecols, squeeze, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, thousands, decimal, comment, skipfooter, convert_float, mangle_dupe_cols, storage_options)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(io, ExcelFile):\n\u001b[1;32m    481\u001b[0m     should_close \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m--> 482\u001b[0m     io \u001b[39m=\u001b[39m ExcelFile(io, storage_options\u001b[39m=\u001b[39;49mstorage_options, engine\u001b[39m=\u001b[39;49mengine)\n\u001b[1;32m    483\u001b[0m \u001b[39melif\u001b[39;00m engine \u001b[39mand\u001b[39;00m engine \u001b[39m!=\u001b[39m io\u001b[39m.\u001b[39mengine:\n\u001b[1;32m    484\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    485\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mEngine should not be specified when passing \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    486\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    487\u001b[0m     )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/io/excel/_base.py:1652\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[1;32m   1650\u001b[0m     ext \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mxls\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1652\u001b[0m     ext \u001b[39m=\u001b[39m inspect_excel_format(\n\u001b[1;32m   1653\u001b[0m         content_or_path\u001b[39m=\u001b[39;49mpath_or_buffer, storage_options\u001b[39m=\u001b[39;49mstorage_options\n\u001b[1;32m   1654\u001b[0m     )\n\u001b[1;32m   1655\u001b[0m     \u001b[39mif\u001b[39;00m ext \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   1656\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1657\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mExcel file format cannot be determined, you must specify \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1658\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39man engine manually.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1659\u001b[0m         )\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/io/excel/_base.py:1525\u001b[0m, in \u001b[0;36minspect_excel_format\u001b[0;34m(content_or_path, storage_options)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(content_or_path, \u001b[39mbytes\u001b[39m):\n\u001b[1;32m   1523\u001b[0m     content_or_path \u001b[39m=\u001b[39m BytesIO(content_or_path)\n\u001b[0;32m-> 1525\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[1;32m   1526\u001b[0m     content_or_path, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m, storage_options\u001b[39m=\u001b[39;49mstorage_options, is_text\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m\n\u001b[1;32m   1527\u001b[0m ) \u001b[39mas\u001b[39;00m handle:\n\u001b[1;32m   1528\u001b[0m     stream \u001b[39m=\u001b[39m handle\u001b[39m.\u001b[39mhandle\n\u001b[1;32m   1529\u001b[0m     stream\u001b[39m.\u001b[39mseek(\u001b[39m0\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/bt4221/lib/python3.9/site-packages/pandas/io/common.py:865\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    856\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(\n\u001b[1;32m    857\u001b[0m             handle,\n\u001b[1;32m    858\u001b[0m             ioargs\u001b[39m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    861\u001b[0m             newline\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    862\u001b[0m         )\n\u001b[1;32m    863\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    864\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 865\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(handle, ioargs\u001b[39m.\u001b[39;49mmode)\n\u001b[1;32m    866\u001b[0m     handles\u001b[39m.\u001b[39mappend(handle)\n\u001b[1;32m    868\u001b[0m \u001b[39m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'neighbourhood_to_region_mapping.xlsx'"
     ]
    }
   ],
   "source": [
    "neighbourhood_to_region_mapping = pd.read_excel('neighbourhood_to_region_mapping.xlsx')\n",
    "neighbourhood_to_region_mapping['neighbourhood'] = neighbourhood_to_region_mapping['neighbourhood'].apply(lambda x: x.replace('\\xa0',''))\n",
    "neighbourhood_to_region_mapping = neighbourhood_to_region_mapping.set_index('neighbourhood').T.to_dict('records')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data['region'] = cleaned_data['neighbourhood'].apply(lambda x: neighbourhood_to_region_mapping[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_data.to_csv('cleaned_restaurant_reviews.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
