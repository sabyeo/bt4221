{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/sabrina/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# for data analytics\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# for visualizations\n",
    "#import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# for data preparation\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "\n",
    "# imblean provides tools for us to deal with imbalanced class sizes\n",
    "from imblearn.over_sampling import SMOTE \n",
    "from imblearn.under_sampling import EditedNearestNeighbours\n",
    "from imblearn.combine import SMOTEENN\n",
    "\n",
    "# # For entropy computation\n",
    "# from pyitlib import discrete_random_variable as drv\n",
    "\n",
    "from scipy import stats\n",
    "import missingno\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# TEXT EDA\n",
    "# from wordcloud import WordCloud\n",
    "\n",
    "# import spacy\n",
    "# from spacy.lang.en.stop_words import STOP_WORDS\n",
    "# from spacy.lang.en import English\n",
    "\n",
    "#import matplotlib.pyplot as plt; plt.rcParams['figure.dpi'] = 100\n",
    "#import matplotlib.gridspec as gridspec\n",
    "#import seaborn as sns; sns.set()\n",
    "import ast\n",
    "\n",
    "from datetime import date, time, datetime\n",
    "import calendar\n",
    "\n",
    "import pandas as pd\n",
    "import emoji\n",
    "import regex as re\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import PorterStemmer\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "\n",
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_reviews = pd.read_csv('../scrape/reviewer-data/all_reviewer_reviews.csv', index_col=0)\n",
    "restaurant_reviews = pd.read_csv('cleaned_restaurant_reviews.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ya Kun Kaya Toast</td>\n",
       "      <td>2 Tampines Central 5, Singapore</td>\n",
       "      <td>\\nBreakfast \\nSet A (S$5.60)\\nKaya butter toas...</td>\n",
       "      <td>3h ago</td>\n",
       "      <td>https://www.burpple.com/ya-kun-kaya-toast-66?b...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>511 Lor Mee</td>\n",
       "      <td>509 Bedok North Street 3, Singapore</td>\n",
       "      <td>\\nLunch\\nLor Mee (S$4.50)\\nKueh tiao topped wi...</td>\n",
       "      <td>5h ago</td>\n",
       "      <td>https://www.burpple.com/511-lor-mee?bp_ref=%2F...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>McDonald's</td>\n",
       "      <td>60 Yishun Avenue 4, Yishun</td>\n",
       "      <td>\\nSupper\\nQuarter Pounder with Cheese Meal (S$...</td>\n",
       "      <td>19h ago</td>\n",
       "      <td>https://www.burpple.com/mcdonalds-555?bp_ref=%...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Tian Yu Tian Fish Head Steamboat</td>\n",
       "      <td>537 Bedok North Street 3, Singapore</td>\n",
       "      <td>\\nZi Char\\nCrispy Noodles (S$4.50)\\n</td>\n",
       "      <td>1d ago</td>\n",
       "      <td>https://www.burpple.com/tian-yu-tian-fish-head...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Kiroi Freshly Baked Cheese Cake</td>\n",
       "      <td>123 Bukit Merah Lane 1, Singapore</td>\n",
       "      <td>\\nCake\\nOriginal (S$9)\\nSoft and pillowy. Japa...</td>\n",
       "      <td>1d ago</td>\n",
       "      <td>https://www.burpple.com/kiroi-freshly-baked-ch...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               name                              address  \\\n",
       "0                 Ya Kun Kaya Toast      2 Tampines Central 5, Singapore   \n",
       "1                       511 Lor Mee  509 Bedok North Street 3, Singapore   \n",
       "2                        McDonald's           60 Yishun Avenue 4, Yishun   \n",
       "3  Tian Yu Tian Fish Head Steamboat  537 Bedok North Street 3, Singapore   \n",
       "4   Kiroi Freshly Baked Cheese Cake    123 Bukit Merah Lane 1, Singapore   \n",
       "\n",
       "                                              review     date  \\\n",
       "0  \\nBreakfast \\nSet A (S$5.60)\\nKaya butter toas...   3h ago   \n",
       "1  \\nLunch\\nLor Mee (S$4.50)\\nKueh tiao topped wi...   5h ago   \n",
       "2  \\nSupper\\nQuarter Pounder with Cheese Meal (S$...  19h ago   \n",
       "3               \\nZi Char\\nCrispy Noodles (S$4.50)\\n   1d ago   \n",
       "4  \\nCake\\nOriginal (S$9)\\nSoft and pillowy. Japa...   1d ago   \n",
       "\n",
       "                                                link    reviewer  keep  \n",
       "0  https://www.burpple.com/ya-kun-kaya-toast-66?b...  alamakgirl     1  \n",
       "1  https://www.burpple.com/511-lor-mee?bp_ref=%2F...  alamakgirl     1  \n",
       "2  https://www.burpple.com/mcdonalds-555?bp_ref=%...  alamakgirl     1  \n",
       "3  https://www.burpple.com/tian-yu-tian-fish-head...  alamakgirl     1  \n",
       "4  https://www.burpple.com/kiroi-freshly-baked-ch...  alamakgirl     1  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewer_reviews.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. reviews: 17773\n",
      "No. reviewers: 10\n"
     ]
    }
   ],
   "source": [
    "# Total Number of restaurant reviews\n",
    "print(f'No. reviews: {len(reviewer_reviews)}')\n",
    "# Number of restaurants with reviews\n",
    "num = len(reviewer_reviews['reviewer'].unique())\n",
    "print(f'No. reviewers: {num}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Restaurants Not Avail in cleaned_restaurant_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove restaurants that are not in available in all_restaurant_reviews \n",
    "unique_restaurants = restaurant_reviews['url'].unique()\n",
    "reviews_available = reviewer_reviews[reviewer_reviews['link'].isin(unique_restaurants)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reviewer</th>\n",
       "      <th>num_reviews_before</th>\n",
       "      <th>num_reviews_available</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DanielFoodDiary</td>\n",
       "      <td>122</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Eatravel</td>\n",
       "      <td>1796</td>\n",
       "      <td>627</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MightyFoodie</td>\n",
       "      <td>1636</td>\n",
       "      <td>465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Vanessa_Kou</td>\n",
       "      <td>1028</td>\n",
       "      <td>326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>3173</td>\n",
       "      <td>606</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>iSteven</td>\n",
       "      <td>46</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>juliuslim</td>\n",
       "      <td>1448</td>\n",
       "      <td>419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>4484</td>\n",
       "      <td>1764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thiampeng</td>\n",
       "      <td>3134</td>\n",
       "      <td>809</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>veronicaphua</td>\n",
       "      <td>906</td>\n",
       "      <td>434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            reviewer  num_reviews_before  num_reviews_available\n",
       "0    DanielFoodDiary                 122                     26\n",
       "1           Eatravel                1796                    627\n",
       "2       MightyFoodie                1636                    465\n",
       "3        Vanessa_Kou                1028                    326\n",
       "4         alamakgirl                3173                    606\n",
       "5            iSteven                  46                     16\n",
       "6          juliuslim                1448                    419\n",
       "7  thefoodcompendium                4484                   1764\n",
       "8          thiampeng                3134                    809\n",
       "9       veronicaphua                 906                    434"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# change in number of reviews\n",
    "result = reviewer_reviews[['reviewer','link']].groupby('reviewer').count().reset_index()\n",
    "result['num_reviews_available'] = list(reviews_available[['reviewer','link']].groupby('reviewer').count()['link'])\n",
    "result.rename({'link':'num_reviews_before'}, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "reviewer                 DanielFoodDiaryEatravelMightyFoodieVanessa_Kou...\n",
       "link                                                                 17773\n",
       "num_reviews_available                                                 5492\n",
       "dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total number of reviews \n",
    "result.sum()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Reviewers with Little Reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>keep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Kazoku Japanese Cuisine</td>\n",
       "      <td>1 Goldhill Plaza, Singapore     ...</td>\n",
       "      <td>\\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>https://www.burpple.com/kazoku-japanese-cuisin...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>Tigerlily Patisserie</td>\n",
       "      <td>350 Joo Chiat Road, Singapore   ...</td>\n",
       "      <td>\\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...</td>\n",
       "      <td>Feb 26 at 12:44pm</td>\n",
       "      <td>https://www.burpple.com/tigerlily-patisserie?b...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Putien (Northpoint City)        ...</td>\n",
       "      <td>930 Yishun Avenue 2, Singapore  ...</td>\n",
       "      <td>\\nBirthday Treat \\n20% discount \\nValid during...</td>\n",
       "      <td>Feb 24 at 10:47pm</td>\n",
       "      <td>https://www.burpple.com/putien-8?bp_ref=%2Ff%2...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Our Tampines Hub Hawker Centre (...</td>\n",
       "      <td>1 Tampines Walk, Singapore      ...</td>\n",
       "      <td>\\nSet C\\nSet C (S$2.50)\\n‘Cos it’s Friday \\nGo...</td>\n",
       "      <td>Feb 24 at 8:33am</td>\n",
       "      <td>https://www.burpple.com/our-tampines-hub?bp_re...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Hokkaido Ramen Santouka (Clarke ...</td>\n",
       "      <td>6 Eu Tong Sen Street, Singapore ...</td>\n",
       "      <td>\\nBirthday Treat\\n50% off Tokusen Toroniku Ram...</td>\n",
       "      <td>Feb 19 at 12:27pm</td>\n",
       "      <td>https://www.burpple.com/hokkaido-ramen-santouk...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4473</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nBaby Spinach Vermicelli 5.5++\\nAgain, light ...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4474</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nChengdu Salivating Chicken 10.8++\\nWow this ...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4475</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nStewed Bamboo Shoots 7.8++\\nWow this wasnt w...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4476</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nDan Dan Mian 8.8++\\nReally restaurant standa...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4477</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nAppetizer (opt Out Basis) 2.6++\\nTheirs is p...</td>\n",
       "      <td>Jan 1, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5450 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "12                  Kazoku Japanese Cuisine               \n",
       "31                     Tigerlily Patisserie               \n",
       "32                  Putien (Northpoint City)        ...   \n",
       "35                  Our Tampines Hub Hawker Centre (...   \n",
       "51                  Hokkaido Ramen Santouka (Clarke ...   \n",
       "...                                                 ...   \n",
       "4473                Paradise Dynasty (Westgate)     ...   \n",
       "4474                Paradise Dynasty (Westgate)     ...   \n",
       "4475                Paradise Dynasty (Westgate)     ...   \n",
       "4476                Paradise Dynasty (Westgate)     ...   \n",
       "4477                Paradise Dynasty (Westgate)     ...   \n",
       "\n",
       "                                                address  \\\n",
       "12                  1 Goldhill Plaza, Singapore     ...   \n",
       "31                  350 Joo Chiat Road, Singapore   ...   \n",
       "32                  930 Yishun Avenue 2, Singapore  ...   \n",
       "35                  1 Tampines Walk, Singapore      ...   \n",
       "51                  6 Eu Tong Sen Street, Singapore ...   \n",
       "...                                                 ...   \n",
       "4473                3 Gateway Drive, Singapore      ...   \n",
       "4474                3 Gateway Drive, Singapore      ...   \n",
       "4475                3 Gateway Drive, Singapore      ...   \n",
       "4476                3 Gateway Drive, Singapore      ...   \n",
       "4477                3 Gateway Drive, Singapore      ...   \n",
       "\n",
       "                                                 review  \\\n",
       "12    \\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...   \n",
       "31    \\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...   \n",
       "32    \\nBirthday Treat \\n20% discount \\nValid during...   \n",
       "35    \\nSet C\\nSet C (S$2.50)\\n‘Cos it’s Friday \\nGo...   \n",
       "51    \\nBirthday Treat\\n50% off Tokusen Toroniku Ram...   \n",
       "...                                                 ...   \n",
       "4473  \\nBaby Spinach Vermicelli 5.5++\\nAgain, light ...   \n",
       "4474  \\nChengdu Salivating Chicken 10.8++\\nWow this ...   \n",
       "4475  \\nStewed Bamboo Shoots 7.8++\\nWow this wasnt w...   \n",
       "4476  \\nDan Dan Mian 8.8++\\nReally restaurant standa...   \n",
       "4477  \\nAppetizer (opt Out Basis) 2.6++\\nTheirs is p...   \n",
       "\n",
       "                                             date  \\\n",
       "12                             4d ago               \n",
       "31                  Feb 26 at 12:44pm               \n",
       "32                  Feb 24 at 10:47pm               \n",
       "35                   Feb 24 at 8:33am               \n",
       "51                  Feb 19 at 12:27pm               \n",
       "...                                           ...   \n",
       "4473                      Jan 2, 2020               \n",
       "4474                      Jan 2, 2020               \n",
       "4475                      Jan 2, 2020               \n",
       "4476                      Jan 2, 2020               \n",
       "4477                      Jan 1, 2020               \n",
       "\n",
       "                                                   link           reviewer  \\\n",
       "12    https://www.burpple.com/kazoku-japanese-cuisin...         alamakgirl   \n",
       "31    https://www.burpple.com/tigerlily-patisserie?b...         alamakgirl   \n",
       "32    https://www.burpple.com/putien-8?bp_ref=%2Ff%2...         alamakgirl   \n",
       "35    https://www.burpple.com/our-tampines-hub?bp_re...         alamakgirl   \n",
       "51    https://www.burpple.com/hokkaido-ramen-santouk...         alamakgirl   \n",
       "...                                                 ...                ...   \n",
       "4473  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "4474  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "4475  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "4476  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "4477  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "\n",
       "      keep  \n",
       "12       1  \n",
       "31       1  \n",
       "32       1  \n",
       "35       1  \n",
       "51       1  \n",
       "...    ...  \n",
       "4473     1  \n",
       "4474     1  \n",
       "4475     1  \n",
       "4476     1  \n",
       "4477     1  \n",
       "\n",
       "[5450 rows x 7 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop danielfooddiary and isteven due to lack of reviews\n",
    "reviewers_to_keep = list(result[result['num_reviews_available']>=100]['reviewer'])\n",
    "final_reviews = reviews_available[reviews_available['reviewer'].isin(reviewers_to_keep)]\n",
    "final_reviews"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean Text Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#decontraction\n",
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(df):\n",
    "    text_list = df['review']\n",
    "\n",
    "    stopwords = nltk.corpus.stopwords.words('english')\n",
    "    new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "    add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "        \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "        'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "        'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "        'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "        'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "        'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "        'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "        'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "        'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "        's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "        've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "        \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "        \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "        'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "    add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "        'it','in','ve','well','could','would','really','also','even',\n",
    "        'alway','always','still','never','much','thing','yet',\n",
    "        'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "        'put','went','seem','order','ordered','give','eat','make','get']\n",
    "    \n",
    "    stopwords.extend(new_stopwords)\n",
    "    stopwords.extend(add_stopwords)\n",
    "    stopwords.extend(add_stopwords_2)\n",
    "\n",
    "    # for sentiment analysis\n",
    "    not_stopwords = {'no','nor','not'} \n",
    "    stopwords = set([word for word in stopwords if word not in not_stopwords])\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    ps = PorterStemmer()\n",
    "    cleaned_text_list = []\n",
    "    count = 0\n",
    "    for text in text_list:\n",
    "\n",
    "        # lower case\n",
    "        text = text.lower()\n",
    "\n",
    "        # demojize    \n",
    "        text = emoji.demojize(text)\n",
    "\n",
    "        # remove headers\n",
    "        text = ' '.join(text.split('\\n')[2:])\n",
    "\n",
    "        # remove location (pushpin or location:)\n",
    "        text = text.split('round_pushpin')[0]\n",
    "        text = text.split('location:')[0]\n",
    "        text = text.split('address:')[0]\n",
    "        text = text.split('address :house_with_garden: :')[0]\n",
    "        text = text.split('address :hut: :')[0]\n",
    "    \n",
    "        # clean punctuation\n",
    "        text = re.sub(r'[^\\w\\s]', '', text)\n",
    "\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "\n",
    "        # remove stopwords and urls\n",
    "        tokens = [word for word in tokens if word not in stopwords]\n",
    "        tokens = [word for word in tokens if 'http' not in word]\n",
    "        tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "        # decontraction\n",
    "        tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "        # translate to english\n",
    "        # text = GoogleTranslator(source='auto', target='en').translate(text)\n",
    "    \n",
    "        # POS tagging\n",
    "        tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "\n",
    "        # lemmatization\n",
    "        tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "            if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "        # concatenate tokens back\n",
    "        cleaned_text = \" \".join(tokens)\n",
    "        cleaned_text_list.append(cleaned_text)\n",
    "\n",
    "        if count%1000 == 0:\n",
    "            print(count)\n",
    "        count+=1\n",
    "\n",
    "    df['cleaned_text'] = cleaned_text_list\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n"
     ]
    }
   ],
   "source": [
    "cleaned_df = clean_text(final_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace empty strings with np.nan\n",
    "cleaned_df = cleaned_df.replace(r'^\\s*$', np.nan, regex=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check for Nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>keep</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>False</th>\n",
       "      <td>5450.0</td>\n",
       "      <td>5446</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5450.0</td>\n",
       "      <td>5361</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         name  address  review    date    link  reviewer    keep  cleaned_text\n",
       "False  5450.0     5446  5450.0  5450.0  5450.0    5450.0  5450.0          5361\n",
       "True      NaN        4     NaN     NaN     NaN       NaN     NaN            89"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if there is any null value\n",
    "cleaned_df.isna().apply(pd.value_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 89 reviews have no review text. Thus, we drop these reviews.\n",
    "cleaned_df = cleaned_df[~cleaned_df['cleaned_text'].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_df.to_csv('cleaned_reviewer_reviews.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
