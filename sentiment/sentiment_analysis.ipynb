{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sabrina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import emoji\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>keep</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kazoku Japanese Cuisine</td>\n",
       "      <td>1 Goldhill Plaza, Singapore     ...</td>\n",
       "      <td>\\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>https://www.burpple.com/kazoku-japanese-cuisin...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>kazoku chirashi s2990 thick slice tuna salmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tigerlily Patisserie</td>\n",
       "      <td>350 Joo Chiat Road, Singapore   ...</td>\n",
       "      <td>\\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...</td>\n",
       "      <td>Feb 26 at 12:44pm</td>\n",
       "      <td>https://www.burpple.com/tigerlily-patisserie?b...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>beehive s15 lemon thyme lychee honey jelly lig...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0                Kazoku Japanese Cuisine               \n",
       "1                   Tigerlily Patisserie               \n",
       "\n",
       "                                             address  \\\n",
       "0                1 Goldhill Plaza, Singapore     ...   \n",
       "1                350 Joo Chiat Road, Singapore   ...   \n",
       "\n",
       "                                              review  \\\n",
       "0  \\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...   \n",
       "1  \\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...   \n",
       "\n",
       "                                          date  \\\n",
       "0                           4d ago               \n",
       "1                Feb 26 at 12:44pm               \n",
       "\n",
       "                                                link    reviewer  keep  \\\n",
       "0  https://www.burpple.com/kazoku-japanese-cuisin...  alamakgirl     1   \n",
       "1  https://www.burpple.com/tigerlily-patisserie?b...  alamakgirl     1   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  kazoku chirashi s2990 thick slice tuna salmon ...  \n",
       "1  beehive s15 lemon thyme lychee honey jelly lig...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviewer_reviews_df = pd.read_csv('../cleaning/cleaned_reviewer_reviews.csv', index_col=0).reset_index(drop=True)\n",
    "reviewer_reviews_df.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        new_data.append(dlist)\n",
    "        # new_list = []\n",
    "        # for token in dlist:\n",
    "        #     word, pos = nltk.pos_tag([token])[0]\n",
    "        #     if pos != 'NN' and pos != 'NNS' and pos != 'NNP' and pos != 'NNPS': #remove nouns that do not determine sentiments\n",
    "        #         new_list.append(word)\n",
    "        # new_data.append(new_list)\n",
    "    return new_data\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.TaggedDocument(doc, [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize reviews\n",
    "tokenized_reviews = nlp_clean(reviewer_reviews_df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1700825, 1942820)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "sentences = bigram_mod[tokenized_reviews]\n",
    "\n",
    "model = gensim.models.Word2Vec(vector_size=1000, min_count=5, alpha=0.025, min_alpha=0.025, seed=123) #### TO TUNE\n",
    "model.build_vocab(sentences)\n",
    "model.train(sentences, total_examples= model.corpus_count, epochs = 10, start_alpha=0.002, end_alpha=-0.016)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kmeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=2, max_iter=1000, random_state=True, n_init=50).fit(X=model.wv.vectors.astype('double'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('make', 0.9999999403953552),\n",
       " ('taste', 0.9999997615814209),\n",
       " ('enjoy', 0.9999997615814209),\n",
       " ('serve', 0.9999997615814209),\n",
       " ('not', 0.9999997019767761),\n",
       " ('egg', 0.9999997019767761),\n",
       " ('cheese', 0.9999997019767761),\n",
       " ('available', 0.9999997019767761),\n",
       " ('rice', 0.9999997019767761),\n",
       " ('sweet', 0.9999996423721313),\n",
       " ('crispy', 0.9999995827674866),\n",
       " ('there', 0.9999995827674866),\n",
       " ('meat', 0.9999995827674866),\n",
       " ('fill', 0.9999995827674866),\n",
       " ('pork', 0.9999995231628418),\n",
       " ('u', 0.9999995231628418),\n",
       " ('sauce', 0.9999995231628418),\n",
       " ('cake', 0.9999995231628418),\n",
       " ('tender', 0.9999995231628418),\n",
       " ('menu', 0.9999995231628418),\n",
       " ('like', 0.9999995231628418),\n",
       " ('dish', 0.9999995231628418),\n",
       " ('flavour', 0.9999995231628418),\n",
       " ('soup', 0.999999463558197),\n",
       " ('quite', 0.999999463558197),\n",
       " ('chef', 0.999999463558197),\n",
       " ('fresh', 0.999999463558197),\n",
       " ('good', 0.999999463558197),\n",
       " ('new', 0.999999463558197),\n",
       " ('texture', 0.999999463558197),\n",
       " ('use', 0.999999463558197),\n",
       " ('didnt', 0.9999994039535522),\n",
       " ('grill', 0.9999994039535522),\n",
       " ('restaurant', 0.9999994039535522),\n",
       " ('love', 0.9999994039535522),\n",
       " ('noodle', 0.9999994039535522),\n",
       " ('food', 0.9999994039535522),\n",
       " ('beef', 0.9999993443489075),\n",
       " ('butter', 0.9999993443489075),\n",
       " ('dessert', 0.9999993443489075),\n",
       " ('mushroom', 0.9999993443489075),\n",
       " ('cream', 0.9999993443489075),\n",
       " ('come', 0.9999992847442627),\n",
       " ('prawn', 0.9999992847442627),\n",
       " ('broth', 0.9999992847442627),\n",
       " ('way', 0.9999992847442627),\n",
       " ('price', 0.9999992847442627),\n",
       " ('take', 0.9999992847442627),\n",
       " ('top', 0.9999992847442627),\n",
       " ('garlic', 0.9999992847442627),\n",
       " ('meal', 0.9999992847442627),\n",
       " ('tea', 0.9999992847442627),\n",
       " ('fry', 0.9999992251396179),\n",
       " ('chicken', 0.9999992251396179),\n",
       " ('wasnt', 0.9999992251396179),\n",
       " ('though', 0.9999992251396179),\n",
       " ('year', 0.9999992251396179),\n",
       " ('time', 0.9999992251396179),\n",
       " ('favourite', 0.9999992251396179),\n",
       " ('sparkle', 0.9999992251396179),\n",
       " ('first', 0.9999992251396179),\n",
       " ('bread', 0.9999992251396179),\n",
       " ('topped', 0.9999992251396179),\n",
       " ('singapore', 0.9999991655349731),\n",
       " ('tomato', 0.9999991655349731),\n",
       " ('side', 0.9999991655349731),\n",
       " ('set', 0.9999991655349731),\n",
       " ('fish', 0.9999991655349731),\n",
       " ('soft', 0.9999991655349731),\n",
       " ('try', 0.9999991655349731),\n",
       " ('cooked', 0.9999991655349731),\n",
       " ('thats', 0.9999991655349731),\n",
       " ('bowl', 0.9999991655349731),\n",
       " ('crunchy', 0.9999991059303284),\n",
       " ('dry', 0.9999991059303284),\n",
       " ('signature', 0.9999991059303284),\n",
       " ('best', 0.9999991059303284),\n",
       " ('bite', 0.9999991059303284),\n",
       " ('strong', 0.9999991059303284),\n",
       " ('light', 0.9999990463256836),\n",
       " ('little', 0.9999990463256836),\n",
       " ('well', 0.9999990463256836),\n",
       " ('item', 0.9999990463256836),\n",
       " ('spice', 0.9999990463256836),\n",
       " ('slice', 0.9999990463256836),\n",
       " ('no', 0.9999990463256836),\n",
       " ('day', 0.9999990463256836),\n",
       " ('dont', 0.9999990463256836),\n",
       " ('great', 0.9999990463256836),\n",
       " ('slightly', 0.9999990463256836),\n",
       " ('since', 0.9999990463256836),\n",
       " ('kind', 0.9999990463256836),\n",
       " ('chocolate', 0.9999990463256836),\n",
       " ('bit', 0.9999990463256836),\n",
       " ('visit', 0.9999989867210388),\n",
       " ('piece', 0.9999989867210388),\n",
       " ('place', 0.9999989867210388),\n",
       " ('drink', 0.9999989867210388),\n",
       " ('classic', 0.9999989867210388),\n",
       " ('add', 0.9999989867210388)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Positive words\n",
    "model.wv.similar_by_vector(kmeans.cluster_centers_[0], topn=100, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('taste', 0.9999997615814209),\n",
       " ('serve', 0.9999997615814209),\n",
       " ('enjoy', 0.9999997019767761),\n",
       " ('not', 0.9999997019767761),\n",
       " ('rice', 0.9999997019767761),\n",
       " ('there', 0.9999997019767761),\n",
       " ('menu', 0.9999997019767761),\n",
       " ('use', 0.9999996423721313),\n",
       " ('grill', 0.9999996423721313),\n",
       " ('love', 0.9999996423721313),\n",
       " ('restaurant', 0.9999996423721313),\n",
       " ('available', 0.9999996423721313),\n",
       " ('make', 0.9999995827674866),\n",
       " ('meat', 0.9999995827674866),\n",
       " ('butter', 0.9999995827674866),\n",
       " ('cheese', 0.9999995231628418),\n",
       " ('chicken', 0.9999995231628418),\n",
       " ('sweet', 0.9999995231628418),\n",
       " ('soup', 0.9999995231628418),\n",
       " ('like', 0.9999995231628418),\n",
       " ('food', 0.9999995231628418),\n",
       " ('flavour', 0.9999995231628418),\n",
       " ('good', 0.999999463558197),\n",
       " ('best', 0.999999463558197),\n",
       " ('quite', 0.999999463558197),\n",
       " ('tender', 0.999999463558197),\n",
       " ('fill', 0.999999463558197),\n",
       " ('dessert', 0.999999463558197),\n",
       " ('fresh', 0.999999463558197),\n",
       " ('mushroom', 0.999999463558197),\n",
       " ('chef', 0.999999463558197),\n",
       " ('texture', 0.999999463558197),\n",
       " ('try', 0.9999994039535522),\n",
       " ('noodle', 0.9999994039535522),\n",
       " ('slice', 0.9999994039535522),\n",
       " ('sauce', 0.9999994039535522),\n",
       " ('cake', 0.9999994039535522),\n",
       " ('bread', 0.9999994039535522),\n",
       " ('favourite', 0.9999993443489075),\n",
       " ('cream', 0.9999993443489075),\n",
       " ('fry', 0.9999993443489075),\n",
       " ('signature', 0.9999993443489075),\n",
       " ('u', 0.9999993443489075),\n",
       " ('topped', 0.9999993443489075),\n",
       " ('egg', 0.9999993443489075),\n",
       " ('time', 0.9999993443489075),\n",
       " ('dish', 0.9999993443489075),\n",
       " ('new', 0.9999993443489075),\n",
       " ('seafood', 0.9999993443489075),\n",
       " ('price', 0.9999992847442627),\n",
       " ('meal', 0.9999992847442627),\n",
       " ('didnt', 0.9999992847442627),\n",
       " ('fish', 0.9999992847442627),\n",
       " ('prawn', 0.9999992847442627),\n",
       " ('pork', 0.9999992847442627),\n",
       " ('come', 0.9999992847442627),\n",
       " ('soft', 0.9999992847442627),\n",
       " ('dry', 0.9999992251396179),\n",
       " ('first', 0.9999992251396179),\n",
       " ('thats', 0.9999992251396179),\n",
       " ('bit', 0.9999992251396179),\n",
       " ('crispy', 0.9999992251396179),\n",
       " ('sparkle', 0.9999991655349731),\n",
       " ('add', 0.9999991655349731),\n",
       " ('beef', 0.9999991655349731),\n",
       " ('take', 0.9999991655349731),\n",
       " ('day', 0.9999991655349731),\n",
       " ('side', 0.9999991655349731),\n",
       " ('tea', 0.9999991655349731),\n",
       " ('piece', 0.9999991059303284),\n",
       " ('top', 0.9999991059303284),\n",
       " ('roll', 0.9999991059303284),\n",
       " ('item', 0.9999991059303284),\n",
       " ('set', 0.9999991059303284),\n",
       " ('salad', 0.9999991059303284),\n",
       " ('though', 0.9999991059303284),\n",
       " ('coffee', 0.9999991059303284),\n",
       " ('bite', 0.9999991059303284),\n",
       " ('well', 0.9999991059303284),\n",
       " ('tomato', 0.9999991059303284),\n",
       " ('light', 0.9999990463256836),\n",
       " ('perfect', 0.9999990463256836),\n",
       " ('small', 0.9999990463256836),\n",
       " ('drink', 0.9999990463256836),\n",
       " ('found', 0.9999990463256836),\n",
       " ('thick', 0.9999990463256836),\n",
       " ('spice', 0.9999990463256836),\n",
       " ('savoury', 0.9999990463256836),\n",
       " ('wasnt', 0.9999990463256836),\n",
       " ('bowl', 0.9999990463256836),\n",
       " ('broth', 0.9999990463256836),\n",
       " ('cooked', 0.9999990463256836),\n",
       " ('year', 0.9999990463256836),\n",
       " ('spicy', 0.9999990463256836),\n",
       " ('look', 0.9999990463256836),\n",
       " ('classic', 0.9999989867210388),\n",
       " ('chocolate', 0.9999989867210388),\n",
       " ('since', 0.9999989867210388),\n",
       " ('garlic', 0.9999989867210388),\n",
       " ('order', 0.9999989867210388)]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# negative words\n",
    "model.wv.similar_by_vector(kmeans.cluster_centers_[1], topn=100, restrict_vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = pd.DataFrame()\n",
    "words['words'] = model.wv.index_to_key # get all words\n",
    "words['vectors'] = words['words'].apply(lambda x: model.wv[x])  # get embeddings for each word \n",
    "words['cluster'] = words['vectors'].apply(lambda x: 1 if kmeans.predict([x])[0]== 0 else -1) # get kmeans cluster for each word\n",
    "words['closeness_score'] = words['vectors'].apply(lambda x: 1/(kmeans.transform([x]).min())) # minimum distance from each word to the center of the cluster. Closer to center = Stronger Positive/Negative Score\n",
    "words['sentiment_coeff'] = words['closeness_score'] * words['cluster'] # negative score = negative sentiment, positive score = positive sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>words</th>\n",
       "      <th>vectors</th>\n",
       "      <th>cluster</th>\n",
       "      <th>closeness_score</th>\n",
       "      <th>sentiment_coeff</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>not</td>\n",
       "      <td>[-0.20506302, 0.5709192, -0.4291008, -0.116481...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.061030</td>\n",
       "      <td>-0.061030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>sauce</td>\n",
       "      <td>[-0.18723677, 0.5244518, -0.39323488, -0.10695...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.070024</td>\n",
       "      <td>-0.070024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>good</td>\n",
       "      <td>[-0.16635017, 0.4629707, -0.34783772, -0.09442...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.086607</td>\n",
       "      <td>-0.086607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flavour</td>\n",
       "      <td>[-0.16395748, 0.45727187, -0.34328416, -0.0924...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.088629</td>\n",
       "      <td>-0.088629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>dish</td>\n",
       "      <td>[-0.16656905, 0.46609807, -0.35059276, -0.0935...</td>\n",
       "      <td>-1</td>\n",
       "      <td>0.085496</td>\n",
       "      <td>-0.085496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6007</th>\n",
       "      <td>moresweetthansmoky</td>\n",
       "      <td>[-0.007788965, 0.021740288, -0.016395465, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.705439</td>\n",
       "      <td>0.705439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6008</th>\n",
       "      <td>anglaise</td>\n",
       "      <td>[-0.0069757467, 0.018018488, -0.013356992, -0....</td>\n",
       "      <td>1</td>\n",
       "      <td>0.610357</td>\n",
       "      <td>0.610357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6009</th>\n",
       "      <td>fung</td>\n",
       "      <td>[-0.005109146, 0.0142147, -0.011513885, -0.002...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.558445</td>\n",
       "      <td>0.558445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6010</th>\n",
       "      <td>tote</td>\n",
       "      <td>[-0.008581543, 0.023909938, -0.017610073, -0.0...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.723859</td>\n",
       "      <td>0.723859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6011</th>\n",
       "      <td>shisenhanten</td>\n",
       "      <td>[-0.019475626, 0.052069962, -0.03965433, -0.00...</td>\n",
       "      <td>1</td>\n",
       "      <td>10.435478</td>\n",
       "      <td>10.435478</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6012 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   words                                            vectors  \\\n",
       "0                    not  [-0.20506302, 0.5709192, -0.4291008, -0.116481...   \n",
       "1                  sauce  [-0.18723677, 0.5244518, -0.39323488, -0.10695...   \n",
       "2                   good  [-0.16635017, 0.4629707, -0.34783772, -0.09442...   \n",
       "3                flavour  [-0.16395748, 0.45727187, -0.34328416, -0.0924...   \n",
       "4                   dish  [-0.16656905, 0.46609807, -0.35059276, -0.0935...   \n",
       "...                  ...                                                ...   \n",
       "6007  moresweetthansmoky  [-0.007788965, 0.021740288, -0.016395465, -0.0...   \n",
       "6008            anglaise  [-0.0069757467, 0.018018488, -0.013356992, -0....   \n",
       "6009                fung  [-0.005109146, 0.0142147, -0.011513885, -0.002...   \n",
       "6010                tote  [-0.008581543, 0.023909938, -0.017610073, -0.0...   \n",
       "6011        shisenhanten  [-0.019475626, 0.052069962, -0.03965433, -0.00...   \n",
       "\n",
       "      cluster  closeness_score  sentiment_coeff  \n",
       "0          -1         0.061030        -0.061030  \n",
       "1          -1         0.070024        -0.070024  \n",
       "2          -1         0.086607        -0.086607  \n",
       "3          -1         0.088629        -0.088629  \n",
       "4          -1         0.085496        -0.085496  \n",
       "...       ...              ...              ...  \n",
       "6007        1         0.705439         0.705439  \n",
       "6008        1         0.610357         0.610357  \n",
       "6009        1         0.558445         0.558445  \n",
       "6010        1         0.723859         0.723859  \n",
       "6011        1        10.435478        10.435478  \n",
       "\n",
       "[6012 rows x 5 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "words[['words', 'sentiment_coeff']].to_csv('sentiment_dictionary.csv', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_map = pd.read_csv('sentiment_dictionary.csv')\n",
    "sentiment_dict = dict(zip(sentiment_map.words.values, sentiment_map.sentiment_coeff.values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>address</th>\n",
       "      <th>review</th>\n",
       "      <th>date</th>\n",
       "      <th>link</th>\n",
       "      <th>reviewer</th>\n",
       "      <th>keep</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kazoku Japanese Cuisine</td>\n",
       "      <td>1 Goldhill Plaza, Singapore     ...</td>\n",
       "      <td>\\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...</td>\n",
       "      <td>4d ago</td>\n",
       "      <td>https://www.burpple.com/kazoku-japanese-cuisin...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>kazoku chirashi s2990 thick slice tuna salmon ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Tigerlily Patisserie</td>\n",
       "      <td>350 Joo Chiat Road, Singapore   ...</td>\n",
       "      <td>\\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...</td>\n",
       "      <td>Feb 26 at 12:44pm</td>\n",
       "      <td>https://www.burpple.com/tigerlily-patisserie?b...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>beehive s15 lemon thyme lychee honey jelly lig...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Putien (Northpoint City)        ...</td>\n",
       "      <td>930 Yishun Avenue 2, Singapore  ...</td>\n",
       "      <td>\\nBirthday Treat \\n20% discount \\nValid during...</td>\n",
       "      <td>Feb 24 at 10:47pm</td>\n",
       "      <td>https://www.burpple.com/putien-8?bp_ref=%2Ff%2...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>20 discount valid birthday month member starte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Our Tampines Hub Hawker Centre (...</td>\n",
       "      <td>1 Tampines Walk, Singapore      ...</td>\n",
       "      <td>\\nSet C\\nSet C (S$2.50)\\n‘Cos it’s Friday \\nGo...</td>\n",
       "      <td>Feb 24 at 8:33am</td>\n",
       "      <td>https://www.burpple.com/our-tampines-hub?bp_re...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>set c s250 co friday s250 cashback pay paylah ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hokkaido Ramen Santouka (Clarke ...</td>\n",
       "      <td>6 Eu Tong Sen Street, Singapore ...</td>\n",
       "      <td>\\nBirthday Treat\\n50% off Tokusen Toroniku Ram...</td>\n",
       "      <td>Feb 19 at 12:27pm</td>\n",
       "      <td>https://www.burpple.com/hokkaido-ramen-santouk...</td>\n",
       "      <td>alamakgirl</td>\n",
       "      <td>1</td>\n",
       "      <td>50 tokusen toroniku ramen s23 s1150 need redee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5356</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nBaby Spinach Vermicelli 5.5++\\nAgain, light ...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "      <td>light somehow satisfy mom love esp sweetness s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5357</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nChengdu Salivating Chicken 10.8++\\nWow this ...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "      <td>wow pretty solid quite faithful classic flavou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5358</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nStewed Bamboo Shoots 7.8++\\nWow this wasnt w...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "      <td>wow wasnt expect lightly quite delicious mild ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5359</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nDan Dan Mian 8.8++\\nReally restaurant standa...</td>\n",
       "      <td>Jan 2, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "      <td>restaurant standard basically play rather safe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5360</th>\n",
       "      <td>Paradise Dynasty (Westgate)     ...</td>\n",
       "      <td>3 Gateway Drive, Singapore      ...</td>\n",
       "      <td>\\nAppetizer (opt Out Basis) 2.6++\\nTheirs is p...</td>\n",
       "      <td>Jan 1, 2020</td>\n",
       "      <td>https://www.burpple.com/paradise-dynasty-10?bp...</td>\n",
       "      <td>thefoodcompendium</td>\n",
       "      <td>1</td>\n",
       "      <td>pretty good tho usually whereas place might as...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5361 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   name  \\\n",
       "0                   Kazoku Japanese Cuisine               \n",
       "1                      Tigerlily Patisserie               \n",
       "2                   Putien (Northpoint City)        ...   \n",
       "3                   Our Tampines Hub Hawker Centre (...   \n",
       "4                   Hokkaido Ramen Santouka (Clarke ...   \n",
       "...                                                 ...   \n",
       "5356                Paradise Dynasty (Westgate)     ...   \n",
       "5357                Paradise Dynasty (Westgate)     ...   \n",
       "5358                Paradise Dynasty (Westgate)     ...   \n",
       "5359                Paradise Dynasty (Westgate)     ...   \n",
       "5360                Paradise Dynasty (Westgate)     ...   \n",
       "\n",
       "                                                address  \\\n",
       "0                   1 Goldhill Plaza, Singapore     ...   \n",
       "1                   350 Joo Chiat Road, Singapore   ...   \n",
       "2                   930 Yishun Avenue 2, Singapore  ...   \n",
       "3                   1 Tampines Walk, Singapore      ...   \n",
       "4                   6 Eu Tong Sen Street, Singapore ...   \n",
       "...                                                 ...   \n",
       "5356                3 Gateway Drive, Singapore      ...   \n",
       "5357                3 Gateway Drive, Singapore      ...   \n",
       "5358                3 Gateway Drive, Singapore      ...   \n",
       "5359                3 Gateway Drive, Singapore      ...   \n",
       "5360                3 Gateway Drive, Singapore      ...   \n",
       "\n",
       "                                                 review  \\\n",
       "0     \\n1-for-1 Don\\nKazoku Chirashi Don (S$29.90++)...   \n",
       "1     \\nBrunch\\nBeehive (S$15+)\\nLemon, thyme and ly...   \n",
       "2     \\nBirthday Treat \\n20% discount \\nValid during...   \n",
       "3     \\nSet C\\nSet C (S$2.50)\\n‘Cos it’s Friday \\nGo...   \n",
       "4     \\nBirthday Treat\\n50% off Tokusen Toroniku Ram...   \n",
       "...                                                 ...   \n",
       "5356  \\nBaby Spinach Vermicelli 5.5++\\nAgain, light ...   \n",
       "5357  \\nChengdu Salivating Chicken 10.8++\\nWow this ...   \n",
       "5358  \\nStewed Bamboo Shoots 7.8++\\nWow this wasnt w...   \n",
       "5359  \\nDan Dan Mian 8.8++\\nReally restaurant standa...   \n",
       "5360  \\nAppetizer (opt Out Basis) 2.6++\\nTheirs is p...   \n",
       "\n",
       "                                             date  \\\n",
       "0                              4d ago               \n",
       "1                   Feb 26 at 12:44pm               \n",
       "2                   Feb 24 at 10:47pm               \n",
       "3                    Feb 24 at 8:33am               \n",
       "4                   Feb 19 at 12:27pm               \n",
       "...                                           ...   \n",
       "5356                      Jan 2, 2020               \n",
       "5357                      Jan 2, 2020               \n",
       "5358                      Jan 2, 2020               \n",
       "5359                      Jan 2, 2020               \n",
       "5360                      Jan 1, 2020               \n",
       "\n",
       "                                                   link           reviewer  \\\n",
       "0     https://www.burpple.com/kazoku-japanese-cuisin...         alamakgirl   \n",
       "1     https://www.burpple.com/tigerlily-patisserie?b...         alamakgirl   \n",
       "2     https://www.burpple.com/putien-8?bp_ref=%2Ff%2...         alamakgirl   \n",
       "3     https://www.burpple.com/our-tampines-hub?bp_re...         alamakgirl   \n",
       "4     https://www.burpple.com/hokkaido-ramen-santouk...         alamakgirl   \n",
       "...                                                 ...                ...   \n",
       "5356  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "5357  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "5358  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "5359  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "5360  https://www.burpple.com/paradise-dynasty-10?bp...  thefoodcompendium   \n",
       "\n",
       "      keep                                       cleaned_text  \n",
       "0        1  kazoku chirashi s2990 thick slice tuna salmon ...  \n",
       "1        1  beehive s15 lemon thyme lychee honey jelly lig...  \n",
       "2        1  20 discount valid birthday month member starte...  \n",
       "3        1  set c s250 co friday s250 cashback pay paylah ...  \n",
       "4        1  50 tokusen toroniku ramen s23 s1150 need redee...  \n",
       "...    ...                                                ...  \n",
       "5356     1  light somehow satisfy mom love esp sweetness s...  \n",
       "5357     1  wow pretty solid quite faithful classic flavou...  \n",
       "5358     1  wow wasnt expect lightly quite delicious mild ...  \n",
       "5359     1  restaurant standard basically play rather safe...  \n",
       "5360     1  pretty good tho usually whereas place might as...  \n",
       "\n",
       "[5361 rows x 8 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = reviewer_reviews_df.copy()\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['tokenized'] = tokenized_reviews\n",
    "results['bigram'] = results['tokenized'].apply(lambda x: bigram_mod[x])\n",
    "results['bigram'] = results['bigram'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(tokenizer=lambda y: y.split(), norm=None)\n",
    "transformed = tfidf.fit_transform(results.bigram)\n",
    "features = pd.Series(tfidf.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tfidf_dictionary(x, transformed_file, features):\n",
    "    tfidf_words = transformed_file[x.name].tocoo() # get the tfidf values for all words in the review text\n",
    "    tfidf_words.col = features.iloc[tfidf_words.col].values # get each word\n",
    "    dictionary = dict(zip(tfidf_words.col, tfidf_words.data)) # dictionary of (word, tfidf value)\n",
    "    return dictionary\n",
    "\n",
    "def replace_tfidf_words(x, transformed_file, features):\n",
    "    dictionary = create_tfidf_dictionary(x, transformed_file, features) \n",
    "    return list(map(lambda y:dictionary[y], x.bigram.split())) # replace each word in the review text with its tfidf value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_tfidf_scores = results.apply(lambda x: replace_tfidf_words(x, transformed, features), axis=1)#this step takes around 3-4 minutes minutes to calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_sentiment_words(word, sentiment_dict):\n",
    "    '''\n",
    "    replacing each word with its associated sentiment score from sentiment dict\n",
    "    '''\n",
    "    try:\n",
    "        out = sentiment_dict[word]\n",
    "    except KeyError:\n",
    "        out = 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "replaced_closeness_scores = results.bigram.apply(lambda x: list(map(lambda y: replace_sentiment_words(y, sentiment_dict), x.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df = pd.DataFrame(data=[replaced_closeness_scores, replaced_tfidf_scores, results.bigram, results.reviewer, results.review, results.link]).T\n",
    "replacement_df.columns = ['sentiment_coeff', 'tfidf_scores', 'sentence', 'reviewer', 'review', 'link']\n",
    "replacement_df['sentiment_rate'] = replacement_df.apply(lambda x: np.array(x.loc['sentiment_coeff']) @ np.array(x.loc['tfidf_scores']), axis=1)\n",
    "replacement_df['prediction'] = (replacement_df.sentiment_rate>0).astype('int8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df.to_csv('sentiments_generated.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy of Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_recommendations = pd.read_csv('../recommendation/doc2vec_content_recommendation.csv', index_col = 0)\n",
    "lda_recommendations = pd.read_csv('../recommendation/lda_content_recommendation.csv', index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "doc2vec_recommendations['recommendations'] = doc2vec_recommendations['recommendations'].apply(lambda x: ast.literal_eval(x))\n",
    "lda_recommendations['recommendations'] = lda_recommendations['recommendations'].apply(lambda x: ast.literal_eval(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accuracy_counts(data):\n",
    "    count = 0\n",
    "    new_rest_count_list = []\n",
    "    old_rest_correct_count_list = []\n",
    "    old_rest_wrong_count_list = []\n",
    "\n",
    "    for idx, row in data.iterrows():\n",
    "        recommendations = row['recommendations']\n",
    "        reviewer = row['reviewer']\n",
    "        reviewer_sentiments = replacement_df[replacement_df['reviewer'] == reviewer]\n",
    "        new_rest_count = 0\n",
    "        old_rest_correct_count = 0\n",
    "        old_rest_wrong_count = 0\n",
    "\n",
    "        for recommendation in recommendations:\n",
    "            result_df = reviewer_sentiments[reviewer_sentiments['link'] == recommendation]\n",
    "            result_df = result_df.drop_duplicates(['link'])\n",
    "            if len(result_df) == 0: # not inside the reviewers reviews. New restaurant\n",
    "                new_rest_count+=1\n",
    "            else:\n",
    "                if int(result_df['prediction']) == 1:\n",
    "                    old_rest_correct_count += 1\n",
    "\n",
    "                else:\n",
    "                    old_rest_wrong_count += 1\n",
    "\n",
    "        new_rest_count_list.append(new_rest_count)\n",
    "        old_rest_correct_count_list.append(old_rest_correct_count)\n",
    "        old_rest_wrong_count_list.append(old_rest_wrong_count)\n",
    "\n",
    "        count +=1\n",
    "        if count%1000 == 0:\n",
    "            print(count)\n",
    "    data['new_rest_count'] = new_rest_count_list\n",
    "    data['old_rest_correct_count'] = old_rest_correct_count_list\n",
    "    data['old_rest_wrong_count'] = old_rest_wrong_count_list\n",
    "\n",
    "    data['old_recomm_count'] = data['old_rest_correct_count']+data['old_rest_wrong_count']\n",
    "    data['perc_correct_counts'] = data['old_rest_correct_count']/data['old_recomm_count']\n",
    "    mean = data['perc_correct_counts'].mean()\n",
    "    data['perc_new'] = data['new_rest_count']/5\n",
    "    mean_new = data['perc_new'].mean()\n",
    "    print(f'Percentage of Correct Counts:{mean}')\n",
    "    print(f'Percentage of New Restaurants:{mean_new}')\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Percentage of Correct Counts:0.7398706577974874\n",
      "Percentage of New Restaurants:0.6086551016601368\n"
     ]
    }
   ],
   "source": [
    "doc2vec_recommendations = get_accuracy_counts(doc2vec_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perc_correct_counts</th>\n",
       "      <th>perc_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eatravel</th>\n",
       "      <td>0.756571</td>\n",
       "      <td>0.629021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MightyFoodie</th>\n",
       "      <td>0.911364</td>\n",
       "      <td>0.600460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanessa_Kou</th>\n",
       "      <td>0.647126</td>\n",
       "      <td>0.866258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alamakgirl</th>\n",
       "      <td>0.728946</td>\n",
       "      <td>0.606601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juliuslim</th>\n",
       "      <td>0.872770</td>\n",
       "      <td>0.652983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thefoodcompendium</th>\n",
       "      <td>0.659996</td>\n",
       "      <td>0.488586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thiampeng</th>\n",
       "      <td>0.707316</td>\n",
       "      <td>0.679208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veronicaphua</th>\n",
       "      <td>0.930151</td>\n",
       "      <td>0.712442</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   perc_correct_counts  perc_new\n",
       "reviewer                                        \n",
       "Eatravel                      0.756571  0.629021\n",
       "MightyFoodie                  0.911364  0.600460\n",
       "Vanessa_Kou                   0.647126  0.866258\n",
       "alamakgirl                    0.728946  0.606601\n",
       "juliuslim                     0.872770  0.652983\n",
       "thefoodcompendium             0.659996  0.488586\n",
       "thiampeng                     0.707316  0.679208\n",
       "veronicaphua                  0.930151  0.712442"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_recommendations.groupby('reviewer')[['perc_correct_counts', 'perc_new']].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "Percentage of Correct Counts:0.7269715854291829\n",
      "Percentage of New Restaurants:0.6030591307591884\n"
     ]
    }
   ],
   "source": [
    "lda_recommendations = get_accuracy_counts(lda_recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>perc_correct_counts</th>\n",
       "      <th>perc_new</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>reviewer</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Eatravel</th>\n",
       "      <td>0.751434</td>\n",
       "      <td>0.662238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MightyFoodie</th>\n",
       "      <td>0.817409</td>\n",
       "      <td>0.532414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Vanessa_Kou</th>\n",
       "      <td>0.627505</td>\n",
       "      <td>0.867485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alamakgirl</th>\n",
       "      <td>0.758138</td>\n",
       "      <td>0.615512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>juliuslim</th>\n",
       "      <td>0.882902</td>\n",
       "      <td>0.653461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thefoodcompendium</th>\n",
       "      <td>0.657882</td>\n",
       "      <td>0.470528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>thiampeng</th>\n",
       "      <td>0.652564</td>\n",
       "      <td>0.690347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>veronicaphua</th>\n",
       "      <td>0.939092</td>\n",
       "      <td>0.706452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   perc_correct_counts  perc_new\n",
       "reviewer                                        \n",
       "Eatravel                      0.751434  0.662238\n",
       "MightyFoodie                  0.817409  0.532414\n",
       "Vanessa_Kou                   0.627505  0.867485\n",
       "alamakgirl                    0.758138  0.615512\n",
       "juliuslim                     0.882902  0.653461\n",
       "thefoodcompendium             0.657882  0.470528\n",
       "thiampeng                     0.652564  0.690347\n",
       "veronicaphua                  0.939092  0.706452"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lda_recommendations.groupby('reviewer')[['perc_correct_counts', 'perc_new']].mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check robustness using VADER sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid_obj = SentimentIntensityAnalyzer()\n",
    "results[\"Sentiment_score\"] = results[\"bigram\"].apply(lambda x: round(sid_obj.polarity_scores(x)['compound'],4))\n",
    "results['Sentiment'] = results['Sentiment_score'].apply(lambda x:  1 if x>0  else 0 if x < 0 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "replacement_df['vader'] = results['Sentiment']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6023130013057265"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(replacement_df[replacement_df['vader'] == replacement_df['prediction']])/len(replacement_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
