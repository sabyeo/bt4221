{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import emoji\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('scrape/restaurant-data/cleaned_restaurant_reviews.csv', index_col=0).reset_index(drop=True)\n",
    "data.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Text Cleaning\n",
    "(to shift to eda code + run with translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "    'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "    's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "    \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "    'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "    'it','in','ve','well','could','would','really','also','even',\n",
    "    'alway','always','still','never','much','thing','yet',\n",
    "    'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "    'put','went','seem','order','ordered','give','eat','make','get']\n",
    "\n",
    "stopwords.extend(new_stopwords)\n",
    "stopwords.extend(add_stopwords)\n",
    "stopwords.extend(add_stopwords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_review_list = []\n",
    "count = 0\n",
    "\n",
    "for review in data['review']:\n",
    "    # lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # deemojize\n",
    "    review = emoji.demojize(review)\n",
    "\n",
    "    # remove headers\n",
    "    review = ' '.join(review.split('\\n')[2:])\n",
    "\n",
    "    # remove location (pushpin or location:)\n",
    "    review = review.split('round_pushpin')[0]\n",
    "    review = review.split('location:')[0]\n",
    "\n",
    "    # clean punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(review)\n",
    "\n",
    "    # remove stopwords and URLs\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [word for word in tokens if 'http' not in word]\n",
    "    tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "    # decontraction\n",
    "    tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "    ### to add in translation code... shld translate each token\n",
    "\n",
    "    ### lemmatization\n",
    "    # POS tagging\n",
    "    tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "            \n",
    "    # lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "        if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "    ### WORD FREQUENCY\n",
    "    # modify count if POS tag is adjective\n",
    "    # new_tokens = []\n",
    "    # for token in tokens:\n",
    "    #     new_tokens.append(token)\n",
    "    #     word, pos = nltk.pos_tag([token])[0]\n",
    "    #     if pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS':  \n",
    "    #         new_tokens.append(token) ### ADD 1 MORE WORD OCCURENCE\n",
    "        #     if word in word_freq.keys():\n",
    "        #         word_freq[word] += 2 ### TO CHANGE THIS PARAMETER (double frequency for adjectivies)\n",
    "        #     else: \n",
    "        #         word_freq[word] = 2\n",
    "        # else:\n",
    "        #     if word in word_freq.keys():\n",
    "        #         word_freq[word] += 1 \n",
    "        #     else: \n",
    "        #         word_freq[word] = 1\n",
    "        \n",
    "    # concatenate tokens back\n",
    "    cleaned_review = \" \".join(tokens)\n",
    "    cleaned_review_list.append(cleaned_review)\n",
    "\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the old cleaned text\n",
    "# data['cleaned_text'] = cleaned_review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each restaurant\n",
    "restaurant_review_df = data[['url', 'cleaned_text']]\n",
    "restaurant_review_df = restaurant_review_df.groupby(['url'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df = restaurant_review_df.join(data.drop(['url', 'cleaned_text'], axis=1), how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        # dlist = list(set(dlist).difference(stopword_set))\n",
    "        new_data.append(dlist)\n",
    "    return new_data\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.TaggedDocument(doc, [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize reviews\n",
    "tokenized_reviews = nlp_clean(restaurant_review_df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "# word embeddings: calculate use frequency -> calculate cosine similarity -> group based on similar usage of words. (LDA just the first step)\n",
    "# vector size 1000 == randomly selecting 1000\n",
    "\n",
    "def doc2vec(tokenized_reviews): #, word_freq\n",
    "    it = LabeledLineSentence(tokenized_reviews, restaurant_review_df['url']) # label reviews with the restaurant url\n",
    "    model = gensim.models.Doc2Vec(vector_size=1000, min_count=5, alpha=0.025, min_alpha=0.025) #### TO TUNE\n",
    "    # model.build_vocab_from_freq(word_freq) #### TRY BUILD VOCAB FROM FREQ\n",
    "    model.build_vocab(it)\n",
    "    model.train(it, total_examples= model.corpus_count, epochs = 1, start_alpha=0.002, end_alpha=-0.016)\n",
    "\n",
    "    print(model.corpus_count) ## check the corpus\n",
    "    print(len(model.docvecs)) # check doc2vec.\n",
    "\n",
    "    ### plot the accuracy of each epoch so we can see if its over or under-fitted\n",
    "    ### tweak learning rate and number of epochs\n",
    "    return it, model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "Base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model = doc2vec(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it.labels_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame()\n",
    "\n",
    "for restaurant in it.labels_list:\n",
    "    similarity_list = []\n",
    "    for restaurant2 in it.labels_list:\n",
    "        similarity_list.append(model.docvecs.similarity(restaurant, restaurant2))\n",
    "    \n",
    "    similarity_df[restaurant] = similarity_list\n",
    "\n",
    "similarity_df.index = list(it.labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimal number of topics\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "    X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-means Accuracy (Silhouette Score)\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "accuracy = []\n",
    "for k in K:\n",
    "    if k == 1:\n",
    "        pass\n",
    "    else: \n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "        X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "        Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "        silhouette_score_average = silhouette_score(model.dv.vectors, kmeans.predict(model.dv.vectors))\n",
    "        accuracy.append(silhouette_score_average)\n",
    "x = list(range(2,len(K)+1))\n",
    "plt.plot(x,accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 6 ### CHANGE ACCORDINGLY\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model.dv.vectors)\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = kmeans.fit_predict(model.dv.vectors)\n",
    "pca = PCA(n_components=2).fit(model.dv.vectors)\n",
    "datapoint = pca.transform(model.dv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "label1 = [\"#FF6961\", \"#FFB480\", \"#F8F38D\", \"#42D6A4\", \"#08CAD1\", \"#59ADF6\", \"#9D94FF\", \"#C780E8\"]\n",
    "color = [label1[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label each restaurant under 1 of the topics\n",
    "restaurant_review_df['topics'] = labels\n",
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of restaurants under each topic\n",
    "restaurant_review_df['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "distances = pairwise_distances(kmeans.cluster_centers_, model.dv.vectors, metric='euclidean')\n",
    "closest_indexes = [np.argpartition(i, 10)[:10] for i in distances] # for each centroid, get the 10 nearest restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df = pd.DataFrame(columns=['topics', 'url', 'cleaned_text'])\n",
    "for idx in closest_indexes:\n",
    "    interpret_df = interpret_df.append(restaurant_review_df[['url', 'cleaned_text', 'topics']].iloc[idx])\n",
    "\n",
    "interpret_df = interpret_df.sort_values(['topics'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Appended additional adjective words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_tokenized_reviews = []\n",
    "for review in tokenized_reviews:\n",
    "    adj_tokenized = []\n",
    "    for token in review:\n",
    "        adj_tokenized.append(token)\n",
    "        word, pos = nltk.pos_tag([token])[0]\n",
    "        if pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS':\n",
    "            adj_tokenized.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model = doc2vec(adj_tokenized_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame()\n",
    "\n",
    "for restaurant in it.labels_list:\n",
    "    similarity_list = []\n",
    "    for restaurant2 in it.labels_list:\n",
    "        similarity_list.append(model.docvecs.similarity(restaurant, restaurant2))\n",
    "    \n",
    "    similarity_df[restaurant] = similarity_list\n",
    "\n",
    "similarity_df.index = list(it.labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OLD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "bigram_reviews = make_bigrams(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model = doc2vec(bigram_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram_mod['tokenized_reviews'])\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[doc] for doc in texts]\n",
    "\n",
    "trigram_reviews = make_trigrams(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model = doc2vec(trigram_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it.labels_list[800]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most similar restaurant to the first restaurant\n",
    "# 'https://www.burpple.com/108-matcha-saro?bp_ref=%2Ff%2F-9cTfRes'\n",
    "model.docvecs.most_similar(800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between 2 documents\n",
    "model.docvecs.similarity(it.labels_list[0], it.labels_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame()\n",
    "\n",
    "for restaurant in it.labels_list:\n",
    "    similarity_list = []\n",
    "    for restaurant2 in it.labels_list:\n",
    "        similarity_list.append(model.docvecs.similarity(restaurant, restaurant2))\n",
    "    \n",
    "    similarity_df[restaurant] = similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.index = list(it.labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df\n",
    "\n",
    "# cosine similarity, jacquard smilarity, euclidean distance, manhatten distance\n",
    "\n",
    "# similarity scores high coz vectors are similar\n",
    "# currently, the vectors are too random. embedding method is not capturing the important data regarding the food items. pre-process dataset to remove words / give more importance to some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.to_csv('doc2vec_restaurant_similarity_append_double.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering using K-Means\n",
    "by default kmeans uses euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimal number of topics\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "    X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### K-means Accuracy (Silhouette Score)\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "accuracy = []\n",
    "for k in K:\n",
    "    if k == 1:\n",
    "        pass\n",
    "    else: \n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "        X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "        Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "        silhouette_score_average = silhouette_score(model.dv.vectors, kmeans.predict(model.dv.vectors))\n",
    "        accuracy.append(silhouette_score_average)\n",
    "x = list(range(2,len(K)+1))\n",
    "plt.plot(x,accuracy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow result shows 8 is the best\n",
    "num_clusters = 6 ## or change to 6?\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model.dv.vectors)\n",
    "labels = kmeans.labels_.tolist()\n",
    "\n",
    "# can get the representative words of each category \n",
    "# cosine similarity measure of the words\n",
    "# LDA uses number of words in each review (prob of word uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = kmeans.fit_predict(model.dv.vectors)\n",
    "pca = PCA(n_components=2).fit(model.dv.vectors)\n",
    "datapoint = pca.transform(model.dv.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "label1 = [\"#FF6961\", \"#FFB480\", \"#F8F38D\", \"#42D6A4\", \"#08CAD1\", \"#59ADF6\", \"#9D94FF\", \"#C780E8\"]\n",
    "color = [label1[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()\n",
    "\n",
    "# if clustering not well, means word embedding is not clearly representing the topic\n",
    "# get a set of food items from wikipedia & set of sentiment words (e.g. good, nice, awful, costly) words that are freuently in reviews\n",
    "# find the frequency of these words in the dataset??\n",
    "# use n-grams?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Clusters\n",
    "identify restaurants that are closest to the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label each restaurant under 1 of the topics\n",
    "restaurant_review_df['topics'] = labels\n",
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of restaurants under each topic\n",
    "restaurant_review_df['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "distances = pairwise_distances(kmeans.cluster_centers_, model.dv.vectors, metric='euclidean')\n",
    "closest_indexes = [np.argpartition(i, 10)[:10] for i in distances] # for each centroid, get the 10 nearest restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df = pd.DataFrame(columns=['topics', 'url', 'cleaned_text'])\n",
    "for idx in closest_indexes:\n",
    "    interpret_df = interpret_df.append(restaurant_review_df[['url', 'cleaned_text', 'topics']].iloc[idx])\n",
    "\n",
    "interpret_df = interpret_df.sort_values(['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df.to_csv('doc2vec_centroid_restaurants_' + str(num_clusters) + '_clusters_append_double.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_bigram.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each topic\n",
    "topic_df = interpret_df[['topics', 'cleaned_text']]\n",
    "topic_df = topic_df.groupby(['topics'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "text = topic_df['cleaned_text']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = topic_df['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_text(text[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_text(text[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[2])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[3])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[4])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[5])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
