{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "      <th>cleaned_categories</th>\n",
       "      <th>...</th>\n",
       "      <th>Western</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Sustainable</th>\n",
       "      <th>Novel</th>\n",
       "      <th>Desserts</th>\n",
       "      <th>Bites</th>\n",
       "      <th>Supper</th>\n",
       "      <th>Breakfast &amp; Brunch</th>\n",
       "      <th>Halal</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nFish Ball Minced Meat Noodle\\nFishball, meat...</td>\n",
       "      <td>Triffany Lim</td>\n",
       "      <td>21m ago</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nOrh lua\\nThere are a couple of stores, but g...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:12pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nPeanut sauce was ace\\nI love a good satay pe...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:10pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nClassic BBQ wings\\nJuicy and tasty like it’s...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:09pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nBBQ stingray\\nIt was yummy but slight warnin...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:08pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url                name  \\\n",
       "0  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "1  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "2  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "3  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "4  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "\n",
       "  neighbourhood    price                                 categories  \\\n",
       "0         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "1         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "2         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "3         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "4         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "\n",
       "                                              review          user  \\\n",
       "0  \\nFish Ball Minced Meat Noodle\\nFishball, meat...  Triffany Lim   \n",
       "1  \\nOrh lua\\nThere are a couple of stores, but g...      Ally Tan   \n",
       "2  \\nPeanut sauce was ace\\nI love a good satay pe...      Ally Tan   \n",
       "3  \\nClassic BBQ wings\\nJuicy and tasty like it’s...      Ally Tan   \n",
       "4  \\nBBQ stingray\\nIt was yummy but slight warnin...      Ally Tan   \n",
       "\n",
       "               date  cleaned_price                     cleaned_categories  \\\n",
       "0           21m ago            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "1  Jul 30 at 4:12pm            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "2  Jul 30 at 4:10pm            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "3  Jul 30 at 4:09pm            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "4  Jul 30 at 4:08pm            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "\n",
       "   ... Western  Recommended  Sustainable  Novel  Desserts  Bites  Supper  \\\n",
       "0  ...       0            0            0      0         0      0       1   \n",
       "1  ...       0            0            0      0         0      0       1   \n",
       "2  ...       0            0            0      0         0      0       1   \n",
       "3  ...       0            0            0      0         0      0       1   \n",
       "4  ...       0            0            0      0         0      0       1   \n",
       "\n",
       "   Breakfast & Brunch  Halal  region  \n",
       "0                   0      0    East  \n",
       "1                   0      0    East  \n",
       "2                   0      0    East  \n",
       "3                   0      0    East  \n",
       "4                   0      0    East  \n",
       "\n",
       "[5 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('scrape/restaurant-data/cleaned_restaurant_reviews.csv', index_col=0).reset_index(drop=True)\n",
    "data.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## New Cleaning Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import emoji\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "    'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "    's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "    \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "    'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "    'it','in','ve','well','could','would','really','also','even',\n",
    "    'alway','always','still','never','much','thing','yet',\n",
    "    'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "    'put','went','seem','order','ordered','give','eat','make','get']\n",
    "\n",
    "stopwords.extend(new_stopwords)\n",
    "stopwords.extend(add_stopwords)\n",
    "stopwords.extend(add_stopwords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "cleaned_review_list = []\n",
    "count = 0\n",
    "\n",
    "for review in data['review']:\n",
    "    # lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # deemojize\n",
    "    review = emoji.demojize(review)\n",
    "\n",
    "    # remove headers\n",
    "    review = ' '.join(review.split('\\n')[2:])\n",
    "\n",
    "    # remove location (pushpin or location:)\n",
    "    review = review.split('round_pushpin')[0]\n",
    "    review = review.split('location:')[0]\n",
    "\n",
    "    # clean punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(review)\n",
    "\n",
    "    # remove stopwords and URLs\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [word for word in tokens if 'http' not in word]\n",
    "    tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "    # decontraction\n",
    "    tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "    ### to add in translation code... shld translate each token\n",
    "\n",
    "    ### lemmatization\n",
    "    # POS tagging\n",
    "    tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "    # lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "    if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "\n",
    "    # concatenate tokens back\n",
    "    cleaned_review = \" \".join(tokens)\n",
    "    cleaned_review_list.append(cleaned_review)\n",
    "\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the old cleaned text\n",
    "data['cleaned_text'] = cleaned_review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each restaurant\n",
    "restaurant_review_df = data[['url', 'cleaned_text']]\n",
    "restaurant_review_df = restaurant_review_df.groupby(['url'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df = restaurant_review_df.join(data.drop(['url', 'cleaned_text'], axis=1), how=\"left\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doc2Vec Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "# stopword_set = set(stopwords.words('english'))\n",
    "\n",
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        # dlist = list(set(dlist).difference(stopword_set))\n",
    "        new_data.append(dlist)\n",
    "    return new_data\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.TaggedDocument(doc, [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize reviews\n",
    "tokenized_reviews = nlp_clean(restaurant_review_df['cleaned_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc2vec(tokenized_reviews):\n",
    "    # label reviews with the restaurant url\n",
    "    it = LabeledLineSentence(tokenized_reviews, restaurant_review_df['url'])\n",
    "    # (it.labels_list[0], it.doc_list[0])\n",
    "    # doc2vec\n",
    "    # word embeddings: calculate use frequency -> calculate cosine similarity -> group based on similar usage of words. (LDA just the first step)\n",
    "    # vector size 1000 == randomly selecting 1000\n",
    "\n",
    "    # pass in our own food dictionary such that the doc2vec generates vectors based on this dictionary\n",
    "    model = gensim.models.Doc2Vec(vector_size=1000, min_count=5, alpha=0.025, min_alpha=0.025) #### TO TUNE\n",
    "    model.build_vocab(it) #### check documentation for this? use build_vocab_from_freq instead?? \n",
    "    model.train(it, total_examples= model.corpus_count, epochs = 10, start_alpha=0.002, end_alpha=-0.016)\n",
    "\n",
    "    print(model.corpus_count) ## check the corpus\n",
    "    print(len(model.docvecs)) # check doc2vec.\n",
    "\n",
    "    ### plot the accuracy of each epoch so we can see if its over or under-fitted\n",
    "    ### tweak learning rate and number of epochs\n",
    "    return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build word frequency\n",
    "word_freq = {}\n",
    "for key in model.wv.index_to_key:\n",
    "    count = model.wv.get_vecattr(key, 'count')\n",
    "\n",
    "    # modify count if POS tag is adjective\n",
    "    word, pos = nltk.pos_tag([key])[0]\n",
    "    if pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS':  \n",
    "       count = count * 2 ### TO CHANGE THIS PARAMETER\n",
    "    \n",
    "    word_freq[key] = count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test model for word frequency\n",
    "it = LabeledLineSentence(tokenized_reviews, restaurant_review_df['url'])\n",
    "model = gensim.models.Doc2Vec(vector_size=1000, min_count=5, alpha=0.025, min_alpha=0.025) #### TO TUNE\n",
    "model.build_vocab_from_freq(word_freq)\n",
    "model.train(it, total_examples= model.corpus_count, epochs = 10, start_alpha=0.002, end_alpha=-0.016)\n",
    "\n",
    "print(model.corpus_count) ## check the corpus\n",
    "print(len(model.docvecs)) # check doc2vec."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec(tokenized_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### bigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "bigram_reviews = make_bigrams(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec(bigram_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram_mod['tokenized_reviews'])\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[doc] for doc in texts]\n",
    "\n",
    "trigram_reviews = make_trigrams(tokenized_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = doc2vec(trigram_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get most similar restaurant to the first restaurant\n",
    "# 'https://www.burpple.com/108-matcha-saro?bp_ref=%2Ff%2F-9cTfRes'\n",
    "model.docvecs.most_similar(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity between 2 documents\n",
    "model.docvecs.similarity(it.labels_list[0], it.labels_list[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df = pd.DataFrame()\n",
    "\n",
    "for restaurant in it.labels_list:\n",
    "    similarity_list = []\n",
    "    for restaurant2 in it.labels_list:\n",
    "        similarity_list.append(model.docvecs.similarity(restaurant, restaurant2))\n",
    "    \n",
    "    similarity_df[restaurant] = similarity_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.index = list(it.labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df\n",
    "\n",
    "# cosine similarity, jacquard smilarity, euclidean distance, manhatten distance\n",
    "\n",
    "# similarity scores high coz vectors are similar\n",
    "# currently, the vectors are too random. embedding method is not capturing the important data regarding the food items. pre-process dataset to remove words / give more importance to some words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_df.to_csv('doc2vec_restaurant_similarity_new_cleaning_bigrams.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Clustering using K-Means\n",
    "by default kmeans uses euclidean distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get optimal number of topics\n",
    "Sum_of_squared_distances = []\n",
    "K = range(1,15)\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "    X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "    Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "plt.xlabel('k')\n",
    "plt.ylabel('Sum_of_squared_distances')\n",
    "plt.title('Elbow Method For Optimal k')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elbow result shows 8 is the best\n",
    "num_clusters = 6 ## or change to 6?\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model.dv.vectors)\n",
    "labels = kmeans.labels_.tolist()\n",
    "\n",
    "# can get the representative words of each category \n",
    "# cosine similarity measure of the words\n",
    "# LDA uses number of words in each review (prob of word uses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l = kmeans.fit_predict(model.docvecs.vectors)\n",
    "pca = PCA(n_components=2).fit(model.docvecs.vectors)\n",
    "datapoint = pca.transform(model.docvecs.vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure\n",
    "label1 = [\"#FF6961\", \"#FFB480\", \"#F8F38D\", \"#42D6A4\", \"#08CAD1\", \"#59ADF6\", \"#9D94FF\", \"#C780E8\"]\n",
    "color = [label1[i] for i in labels]\n",
    "plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "centroids = kmeans.cluster_centers_\n",
    "centroidpoint = pca.transform(centroids)\n",
    "plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "plt.show()\n",
    "\n",
    "# if clustering not well, means word embedding is not clearly representing the topic\n",
    "# get a set of food items from wikipedia & set of sentiment words (e.g. good, nice, awful, costly) words that are freuently in reviews\n",
    "# find the frequency of these words in the dataset??\n",
    "# use n-grams?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# label each restaurant under 1 of the topics\n",
    "restaurant_review_df['topics'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count of restaurants under each topic\n",
    "restaurant_review_df['topics'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret Clusters\n",
    "identify restaurants that are closest to the centroid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "distances = pairwise_distances(kmeans.cluster_centers_, model.docvecs.doctag_syn0, metric='euclidean')\n",
    "closest_indexes = [np.argpartition(i, 10)[:10] for i in distances] # for each centroid, get the 10 nearest restaurants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df = pd.DataFrame(columns=['topics', 'url', 'cleaned_text'])\n",
    "for idx in closest_indexes:\n",
    "    interpret_df = interpret_df.append(restaurant_review_df[['url', 'cleaned_text', 'topics']].iloc[idx])\n",
    "\n",
    "interpret_df = interpret_df.sort_values(['topics'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df.to_csv('doc2vec_centroid_restaurants_' + str(num_clusters) + '_clusters_new_cleaning.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_bigram.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each topic\n",
    "topic_df = interpret_df[['topics', 'cleaned_text']]\n",
    "topic_df = topic_df.groupby(['topics'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "text = topic_df['cleaned_text']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = topic_df['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_text(text[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_text(text[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[2])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[3])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[4])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[5])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[6])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white').generate_from_frequencies(df[7])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
