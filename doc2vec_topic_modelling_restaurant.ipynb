{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/natalie/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "import gensim\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import metrics\n",
    "import pylab as pl\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import emoji\n",
    "import regex as re\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.metrics import silhouette_samples, silhouette_score\n",
    "from sklearn.metrics import pairwise_distances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>name</th>\n",
       "      <th>neighbourhood</th>\n",
       "      <th>price</th>\n",
       "      <th>categories</th>\n",
       "      <th>review</th>\n",
       "      <th>user</th>\n",
       "      <th>date</th>\n",
       "      <th>cleaned_price</th>\n",
       "      <th>cleaned_categories</th>\n",
       "      <th>...</th>\n",
       "      <th>Western</th>\n",
       "      <th>Recommended</th>\n",
       "      <th>Sustainable</th>\n",
       "      <th>Novel</th>\n",
       "      <th>Desserts</th>\n",
       "      <th>Bites</th>\n",
       "      <th>Supper</th>\n",
       "      <th>Breakfast &amp; Brunch</th>\n",
       "      <th>Halal</th>\n",
       "      <th>region</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nFish Ball Minced Meat Noodle\\nFishball, meat...</td>\n",
       "      <td>Triffany Lim</td>\n",
       "      <td>21m ago</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.burpple.com/bedok-85-market?bp_ref...</td>\n",
       "      <td>85 Fengshan Centre</td>\n",
       "      <td>Bedok</td>\n",
       "      <td>~$5/pax</td>\n",
       "      <td>['Hawker Food', 'Supper', 'Cheap &amp; Good']</td>\n",
       "      <td>\\nOrh lua\\nThere are a couple of stores, but g...</td>\n",
       "      <td>Ally Tan</td>\n",
       "      <td>Jul 30 at 4:12pm</td>\n",
       "      <td>5.0</td>\n",
       "      <td>['Local Delights', 'Supper', 'Value']</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>East</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 44 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url                name  \\\n",
       "0  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "1  https://www.burpple.com/bedok-85-market?bp_ref...  85 Fengshan Centre   \n",
       "\n",
       "  neighbourhood    price                                 categories  \\\n",
       "0         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "1         Bedok  ~$5/pax  ['Hawker Food', 'Supper', 'Cheap & Good']   \n",
       "\n",
       "                                              review          user  \\\n",
       "0  \\nFish Ball Minced Meat Noodle\\nFishball, meat...  Triffany Lim   \n",
       "1  \\nOrh lua\\nThere are a couple of stores, but g...      Ally Tan   \n",
       "\n",
       "               date  cleaned_price                     cleaned_categories  \\\n",
       "0           21m ago            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "1  Jul 30 at 4:12pm            5.0  ['Local Delights', 'Supper', 'Value']   \n",
       "\n",
       "   ... Western  Recommended  Sustainable  Novel  Desserts  Bites  Supper  \\\n",
       "0  ...       0            0            0      0         0      0       1   \n",
       "1  ...       0            0            0      0         0      0       1   \n",
       "\n",
       "   Breakfast & Brunch  Halal  region  \n",
       "0                   0      0    East  \n",
       "1                   0      0    East  \n",
       "\n",
       "[2 rows x 44 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('scrape/restaurant-data/cleaned_restaurant_reviews.csv', index_col=0).reset_index(drop=True)\n",
    "data.head(2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Text Cleaning\n",
    "(to shift to eda code + run with translation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decontracted(phrase):\n",
    "    # specific\n",
    "    phrase = re.sub(r\"won\\'t\", \"will not\", phrase)\n",
    "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
    "    phrase = re.sub(r\"didn\\'t\", \"did not\", phrase)\n",
    "    phrase = re.sub(r\"don\\'t\", \"do not\", phrase)\n",
    "    phrase = re.sub(r\"o\\'clock\", \"clock\", phrase)\n",
    "    phrase = re.sub(r\"couldn\\'t\", \"could not\", phrase)\n",
    "    phrase = re.sub(r\"that\\'s\", \"that is\", phrase)       \n",
    "    phrase = re.sub(r\"go-around\", \"go around\", phrase)  \n",
    "    # general\n",
    "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
    "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
    "    #phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
    "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
    "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
    "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
    "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
    "    return phrase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "new_stopwords = ['address', 'note', 'tel', 'website', 'open', 'burpple']\n",
    "add_stopwords = set(['br', 'the', 'i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\",\\\n",
    "    \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', \\\n",
    "    'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their',\\\n",
    "    'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', \\\n",
    "    'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', \\\n",
    "    'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', \\\n",
    "    'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after',\\\n",
    "    'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further',\\\n",
    "    'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\\\n",
    "    'most', 'other', 'some', 'such', 'only', 'own', 'same', 'so', 'than', 'too', 'very', \\\n",
    "    's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', \\\n",
    "    've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn',\\\n",
    "    \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn',\\\n",
    "    \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", \\\n",
    "    'won', \"won't\", 'wouldn', \"wouldn't\"])\n",
    "add_stopwords_2 =  ['n','s','m','i','1','2','3','4','5','6','7','8','9','10','one','two',\n",
    "    'it','in','ve','well','could','would','really','also','even',\n",
    "    'alway','always','still','never','much','thing','yet',\n",
    "    'said','asked','did','go','got','do','make','know','think','come','going',\n",
    "    'put','went','seem','order','ordered','give','eat','make','get']\n",
    "\n",
    "stopwords.extend(new_stopwords)\n",
    "stopwords.extend(add_stopwords)\n",
    "stopwords.extend(add_stopwords_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n"
     ]
    }
   ],
   "source": [
    "cleaned_review_list = []\n",
    "count = 0\n",
    "\n",
    "for review in data['review']:\n",
    "    # lower case\n",
    "    review = review.lower()\n",
    "\n",
    "    # deemojize\n",
    "    review = emoji.demojize(review)\n",
    "\n",
    "    # remove headers\n",
    "    review = ' '.join(review.split('\\n')[2:])\n",
    "\n",
    "    # remove location (pushpin or location:)\n",
    "    review = review.split('round_pushpin')[0]\n",
    "    review = review.split('location:')[0]\n",
    "\n",
    "    # clean punctuation\n",
    "    review = re.sub(r'[^\\w\\s]', '', review)\n",
    "\n",
    "    # tokenize\n",
    "    tokens = word_tokenize(review)\n",
    "\n",
    "    # remove stopwords and URLs\n",
    "    tokens = [word for word in tokens if word not in stopwords]\n",
    "    tokens = [word for word in tokens if 'http' not in word]\n",
    "    tokens = [word for word in tokens if 'www' not in word]\n",
    "\n",
    "    # decontraction\n",
    "    tokens = [decontracted(word) for word in tokens]\n",
    "\n",
    "    ### to add in translation code... shld translate each token\n",
    "\n",
    "    ### lemmatization\n",
    "    # POS tagging\n",
    "    tokens = [nltk.pos_tag([word]) for word in tokens]\n",
    "            \n",
    "    # lemmatization\n",
    "    tokens = [lemmatizer.lemmatize(word[0][0], get_wordnet_pos(word[0][1])) \n",
    "        if get_wordnet_pos(word[0][1])!=None else lemmatizer.lemmatize(word[0][0]) for word in tokens]\n",
    "    \n",
    "    ### WORD FREQUENCY\n",
    "    # modify count if POS tag is adjective\n",
    "    # new_tokens = []\n",
    "    # for token in tokens:\n",
    "    #     new_tokens.append(token)\n",
    "    #     word, pos = nltk.pos_tag([token])[0]\n",
    "    #     if pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS':  \n",
    "    #         new_tokens.append(token) ### ADD 1 MORE WORD OCCURENCE\n",
    "        #     if word in word_freq.keys():\n",
    "        #         word_freq[word] += 2 ### TO CHANGE THIS PARAMETER (double frequency for adjectivies)\n",
    "        #     else: \n",
    "        #         word_freq[word] = 2\n",
    "        # else:\n",
    "        #     if word in word_freq.keys():\n",
    "        #         word_freq[word] += 1 \n",
    "        #     else: \n",
    "        #         word_freq[word] = 1\n",
    "        \n",
    "    # concatenate tokens back\n",
    "    cleaned_review = \" \".join(tokens)\n",
    "    cleaned_review_list.append(cleaned_review)\n",
    "\n",
    "    if count%1000 == 0:\n",
    "        print(count)\n",
    "    count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the old cleaned text\n",
    "data['cleaned_text'] = cleaned_review_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each restaurant\n",
    "restaurant_review_df = data[['url', 'cleaned_text']]\n",
    "restaurant_review_df = restaurant_review_df.groupby(['url'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "restaurant_review_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_characteristics_df = data.drop(['review', 'cleaned_text'], axis=1)\n",
    "restaurant_characteristics_df = restaurant_characteristics_df.groupby(['url'], as_index = False).first()\n",
    "restaurant_characteristics_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df = pd.merge(restaurant_review_df, restaurant_characteristics_df, on='url')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "def nlp_clean(data):\n",
    "    new_data = []\n",
    "    for d in data:\n",
    "        new_str = d.lower()\n",
    "        dlist = tokenizer.tokenize(new_str)\n",
    "        new_data.append(dlist)\n",
    "    return new_data\n",
    "\n",
    "class LabeledLineSentence(object):\n",
    "    def __init__(self, doc_list, labels_list):\n",
    "        self.labels_list = labels_list\n",
    "        self.doc_list = doc_list\n",
    "    def __iter__(self):\n",
    "        for idx, doc in enumerate(self.doc_list):\n",
    "              yield gensim.models.doc2vec.TaggedDocument(doc, [self.labels_list[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize reviews\n",
    "tokenized_reviews = nlp_clean(restaurant_review_df['cleaned_text'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# doc2vec\n",
    "# word embeddings: calculate use frequency -> calculate cosine similarity -> group based on similar usage of words. (LDA just the first step)\n",
    "# vector size 1000 == randomly selecting 1000\n",
    "\n",
    "def doc2vec(tokenized_reviews, num_epochs): #, word_freq\n",
    "    it = LabeledLineSentence(tokenized_reviews, restaurant_review_df['url']) # label reviews with the restaurant url\n",
    "    model = gensim.models.Doc2Vec(vector_size=1000, min_count=5, alpha=0.025, min_alpha=0.025, seed=123) #### TO TUNE\n",
    "    # model.build_vocab_from_freq(word_freq) #### TRY BUILD VOCAB FROM FREQ\n",
    "    model.build_vocab(it)\n",
    "    model.train(it, total_examples= model.corpus_count, epochs = num_epochs, start_alpha=0.002, end_alpha=-0.016)\n",
    "\n",
    "    # print(model.corpus_count) ## check the corpus\n",
    "    # print(len(model.docvecs)) # check doc2vec.\n",
    "\n",
    "    ### plot the accuracy of each epoch so we can see if its over or under-fitted\n",
    "    ### tweak learning rate and number of epochs\n",
    "    return it, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimise_epochs(tokenized_reviews, max_epoch):\n",
    "    range_epochs = range(2,max_epoch+1)\n",
    "    accuracy = []\n",
    "    for num_epoch in range_epochs:\n",
    "        it, model = doc2vec(tokenized_reviews, num_epoch)\n",
    "\n",
    "        kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=100) ### ASSUME 5 clusters\n",
    "        X = kmeans.fit(model.dv.vectors)\n",
    "        silhouette_score_average = silhouette_score(model.dv.vectors, kmeans.predict(model.dv.vectors))\n",
    "        accuracy.append(silhouette_score_average)\n",
    "\n",
    "    plt.plot(list(range_epochs),accuracy)\n",
    "    plt.title('Silhouette Score For Each Epoch')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_silhouette(tokenized_reviews):\n",
    "    it, model = doc2vec(tokenized_reviews, 10)\n",
    "\n",
    "    kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=100) ### ASSUME 5 clusters\n",
    "    X = kmeans.fit(model.dv.vectors)\n",
    "    silhouette_score_average = silhouette_score(model.dv.vectors, kmeans.predict(model.dv.vectors))\n",
    "    return silhouette_score_average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_matrix(model):\n",
    "    similarity_df = pd.DataFrame()\n",
    "\n",
    "    for restaurant in it.labels_list:\n",
    "        similarity_list = []\n",
    "        for restaurant2 in it.labels_list:\n",
    "            similarity_list.append(model.docvecs.similarity(restaurant, restaurant2))\n",
    "        \n",
    "        similarity_df[restaurant] = similarity_list\n",
    "\n",
    "    similarity_df.index = list(it.labels_list)\n",
    "\n",
    "    return similarity_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_topics_elbow(model):\n",
    "    # get optimal number of topics\n",
    "    Sum_of_squared_distances = []\n",
    "    K = range(1,15)\n",
    "    for k in K:\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "        X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "        Sum_of_squared_distances.append(kmeans.inertia_)\n",
    "\n",
    "    plt.plot(K, Sum_of_squared_distances, 'bx-')\n",
    "    plt.xlabel('k')\n",
    "    plt.ylabel('Sum_of_squared_distances')\n",
    "    plt.title('Elbow Method For Optimal k')\n",
    "    plt.show()\n",
    "\n",
    "def num_topics_silhouette(model, min_topic, max_topic): # min topic should be the first elbow observed\n",
    "    ### K-means Accuracy (Silhouette Score) (assume minimum of 5 clusters)\n",
    "    accuracy = []\n",
    "    for k in range(min_topic,max_topic+1):\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=100)\n",
    "        X = kmeans.fit(model.dv.vectors) ###using documnet vecter numbers. doctag_syn0 calls doc2vec that is trained earlier.\n",
    "        silhouette_score_average = silhouette_score(model.dv.vectors, kmeans.predict(model.dv.vectors))\n",
    "        accuracy.append(silhouette_score_average)\n",
    "    x = list(range(min_topic,max_topic+1))\n",
    "    print(accuracy)\n",
    "    plt.plot(x,accuracy)\n",
    "    plt.title('Silhouette Score per K')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_plot(model):\n",
    "    l = kmeans.fit_predict(model.dv.vectors)\n",
    "    pca = PCA(n_components=2).fit(model.dv.vectors)\n",
    "    datapoint = pca.transform(model.dv.vectors)\n",
    "\n",
    "    plt.figure\n",
    "    label1 = [\"#FF6961\", \"#FFB480\", \"#F8F38D\", \"#42D6A4\", \"#08CAD1\", \"#59ADF6\", \"#9D94FF\", \"#C780E8\"]\n",
    "    #label1 = [\"#FF355E\", \"#FD5B78\", \"#FF6037\", \"#FF9966\", \"#FF9933\", \"#FFCC33\", \"#FFFF66\", \"#CCFF00\", \"#66FF66\", \"#AAF0D1\", \"#16D0CB\", \"#50BFE6\", \"#9C27B0\", \"#EE34D2\", \"#FF00CC\"]\n",
    "    color = [label1[i] for i in labels]\n",
    "    plt.scatter(datapoint[:, 0], datapoint[:, 1], c=color)\n",
    "\n",
    "    centroids = kmeans.cluster_centers_\n",
    "    centroidpoint = pca.transform(centroids)\n",
    "    plt.scatter(centroidpoint[:, 0], centroidpoint[:, 1], marker='^', s=150, c='#000000')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nearest_restaurants_to_centroid(model):\n",
    "    # label each restaurant under 1 of the topics\n",
    "    restaurant_review_df['topics'] = labels\n",
    "\n",
    "    # count of restaurants under each topic\n",
    "    print(restaurant_review_df['topics'].value_counts())\n",
    "\n",
    "    distances = pairwise_distances(kmeans.cluster_centers_, model.dv.vectors, metric='euclidean')\n",
    "    closest_indexes = [np.argpartition(i, 10)[:10] for i in distances] # for each centroid, get the 10 nearest restaurants\n",
    "\n",
    "    interpret_df = pd.DataFrame(columns=['topics', 'url', 'name', 'cleaned_text'])\n",
    "    for idx in closest_indexes:\n",
    "        interpret_df = interpret_df.append(restaurant_review_df[['url', 'name', 'cleaned_text', 'topics']].iloc[idx])\n",
    "\n",
    "    interpret_df = interpret_df.sort_values(['topics'])\n",
    "    return restaurant_review_df, interpret_df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1\n",
    "Base model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify optimal number of epochs\n",
    "# plot k means silhouette score for each doc2vec epoch\n",
    "optimise_epochs(tokenized_reviews, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model1 = doc2vec(tokenized_reviews, 3) ### Change number of epochs accordingly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix(model1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_elbow(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_silhouette(model1, 3, 8) ### CHANGE ACCORDINGLY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4 ### CHANGE ACCORDINGLY \n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model1.dv.vectors) ### CHANGE MODEL NUMBER\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(model1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df, interpret_df = nearest_restaurants_to_centroid(model1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)\n",
    "\n",
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)\n",
    "\n",
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_model1.csv') ### CHANGE MODEL NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "Appended additional adjective words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_adj_tokenized_reviews(n):\n",
    "    adj_tokenized_reviews = []\n",
    "    for review in tokenized_reviews:\n",
    "        adj_tokenized = []\n",
    "        for token in review:\n",
    "            adj_tokenized.append(token)\n",
    "            word, pos = nltk.pos_tag([token])[0]\n",
    "            if pos == 'JJ' or pos == 'JJR' or pos == 'JJS' or pos == 'RB' or pos == 'RBR' or pos == 'RBS':\n",
    "                for i in range(n):\n",
    "                    adj_tokenized.append(token)\n",
    "                \n",
    "        adj_tokenized_reviews.append(adj_tokenized)\n",
    "    return adj_tokenized_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj_tokenized_reviews = create_adj_tokenized_reviews(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(adj_tokenized_reviews[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_reviews[0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify optimal weights. Assume 5 clusters, 10 epochs \n",
    "scores = []\n",
    "range_weights = range(1,4)\n",
    "for i in range_weights:\n",
    "    adj_tokenized_reviews = create_adj_tokenized_reviews(i)\n",
    "    scores.append(plot_silhouette(adj_tokenized_reviews))\n",
    "\n",
    "plt.plot(list(range_weights),scores)\n",
    "plt.title('Silhouette Score For Each Added Frequencies')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify optimal number of epochs\n",
    "adj_tokenized_reviews = create_adj_tokenized_reviews(2)\n",
    "optimise_epochs(adj_tokenized_reviews, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model2 = doc2vec(adj_tokenized_reviews, 4) ### Change number of epochs accordingly"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix(model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_elbow(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_silhouette(model2, 3, 8) ### CHANGE ACCORDINGLY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3 ### CHANGE ACCORDINGLY\n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model2.dv.vectors) ### CHANGE MODEL NUMBER\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(model2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df, interpret_df = nearest_restaurants_to_centroid(model2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)\n",
    "\n",
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)\n",
    "\n",
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_model2.csv') ### CHANGE MODEL NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "Bi-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "bigram_reviews = make_bigrams(tokenized_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify optimal number of epochs\n",
    "# plot k means silhouette score for each doc2vec epoch\n",
    "optimise_epochs(bigram_reviews, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model3 = doc2vec(bigram_reviews, 6) ### Change number of epochs accordingly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix(model3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_elbow(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_silhouette(model3, 3, 8) ### CHANGE ACCORDINGLY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 3 ### CHANGE ACCORDINGLY \n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model3.dv.vectors) ### CHANGE MODEL NUMBER\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(model3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df, interpret_df = nearest_restaurants_to_centroid(model3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)\n",
    "\n",
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)\n",
    "\n",
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_model3.csv') ### CHANGE MODEL NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "Tri-gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.phrases import Phrases, Phraser\n",
    "# Build the bigram and trigram models\n",
    "bigram = gensim.models.Phrases(tokenized_reviews) # higher threshold fewer phrases.\n",
    "\n",
    "# Faster way to get a sentence clubbed as a trigram/bigram\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "\n",
    "trigram = gensim.models.Phrases(bigram_mod['tokenized_reviews'])\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)\n",
    "\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[doc] for doc in texts]\n",
    "\n",
    "trigram_reviews = make_trigrams(tokenized_reviews)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify optimal number of epochs\n",
    "# plot k means silhouette score for each doc2vec epoch\n",
    "optimise_epochs(trigram_reviews, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "it, model4 = doc2vec(trigram_reviews, 3) ### Change number of epochs accordingly"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix(model4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Clustering Using K-Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_elbow(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_topics_silhouette(model4, 3, 8) ### CHANGE ACCORDINGLY"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = 4 ### CHANGE ACCORDINGLY \n",
    "kmeans = KMeans(n_clusters=num_clusters, init='k-means++', max_iter=100) \n",
    "X = kmeans.fit(model4.dv.vectors) ### CHANGE MODEL NUMBER\n",
    "labels = kmeans.labels_.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_plot(model4)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df, interpret_df = nearest_restaurants_to_centroid(model4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interpret_df[interpret_df['topics'] == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "restaurant_review_df_encoded = restaurant_review_df.drop(['name', 'cleaned_text', 'price', 'categories', 'review', 'user', 'date', 'cleaned_categories'], axis=1)\n",
    "\n",
    "# one hot encode region & topics\n",
    "region_dummies = pd.get_dummies(restaurant_review_df_encoded['region'])\n",
    "neighbourhood_dummies = pd.get_dummies(restaurant_review_df_encoded['neighbourhood'])\n",
    "topic_dummies = pd.get_dummies(restaurant_review_df_encoded['topics'])\n",
    "\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.drop(['region', 'neighbourhood', 'topics'], axis=1)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(region_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(neighbourhood_dummies)\n",
    "restaurant_review_df_encoded = restaurant_review_df_encoded.join(topic_dummies)\n",
    "\n",
    "restaurant_review_df_encoded.to_csv('doc2vec_labelled_restaurants_model4.csv') ### CHANGE MODEL NAME"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (IGNORE) WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all the reviews for each topic\n",
    "topic_df = interpret_df[['topics', 'cleaned_text']]\n",
    "topic_df = topic_df.groupby(['topics'], as_index = False).agg({'cleaned_text': ' '.join})\n",
    "topic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF\n",
    "text = topic_df['cleaned_text']\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(text)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "dense = X.todense()\n",
    "denselist = dense.tolist()\n",
    "df = pd.DataFrame(denselist, columns=feature_names).transpose()\n",
    "df.columns = topic_df['topics']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_text(text[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[0])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_text(text[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[1])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[2])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[3])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[4])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(background_color = 'white', collocations = False).generate_from_frequencies(df[5])\n",
    "# Display the generated Word Cloud\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bt4221",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
